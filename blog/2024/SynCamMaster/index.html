<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints | Patrick Kwon (권용재) </title> <meta name="author" content="Patrick Kwon"> <meta name="description" content="Academic Portfolio webpage for Patrick Kwon (권용재), Deep learning Researcher at Naver Webtoon "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yj7082126.github.io/blog/2024/SynCamMaster/"> <script src="/assets/js/theme.js?v=9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Patrick Kwon (권용재) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</h1> <p class="post-meta"> Created in December 15, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/video-generation"> <i class="fa-solid fa-hashtag fa-sm"></i> Video Generation</a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="syncammaster">SynCamMaster</h2> <table> <thead> <tr> <th>Name</th> <th>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</th> </tr> </thead> <tbody> <tr> <td>Authors</td> <td>Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</td> </tr> <tr> <td>Institute</td> <td>Zhejiang University, Kuaishou Technology, Tsinghua University, CUHK</td> </tr> <tr> <td>Conference</td> <td>arxiv (2412.07760)</td> </tr> <tr> <td>Links</td> <td> <a href="https://arxiv.org/abs/2412.07760" rel="external nofollow noopener" target="_blank">[ArXiV]</a> <a href="https://jianhongbai.github.io/SynCamMaster/" rel="external nofollow noopener" target="_blank">[Project]</a> <a href="https://github.com/KwaiVGI/SynCamMaster" rel="external nofollow noopener" target="_blank">[Code]</a> </td> </tr> <tr> <td>Tl;dr</td> <td>복수의 카메라 구도에 대해 똑같은 내용이 담긴 영상을 생성하는 기술</td> </tr> </tbody> </table> <p><img src="/assets/img/20241215/image.png" alt="image.png"></p> <h2 id="정리">정리</h2> <p>| 무엇을 하고자 하는가 | 다중 카메라 구도에서부터의 영상 생성 | | — | — | | 왜 이전에는 해결되지 않았나 | 1. 복수의 카메라 구도에서부터의 똑같은 내용에 대한 영상 생성을, 4D 일관성을 지키면서 생성하는것</p> <ol> <li>1을 이루기 위한, 다양한 다중 카메라 영상 데이터셋의 부재 | | 어떻게 해결하였나 | 거대 T2V 모델에 기반하며, 다양한 어댑터 모델들을 적용 <ul> <li> <table> <tbody> <tr> <td>Camera Encoder : 각 카메라들의 [R</td> <td>t] 입력을 인코딩</td> </tr> </tbody> </table> </li> <li>Multi-view Synchronization Model : DiT 의 각 Transformer 레이어에 부착, 4D Consistency 를 유지한다</li> </ul> </li> </ol> <p>신종 데이터셋 제작 : 언리얼 엔진 기반의 다중 카메라 영상 데이터셋 + 공개되어있는 다중 이미지, 단일 영상등으로 구성</p> <ul> <li> <table> <tbody> <tr> <td>Hybrid Data-training 으로 훈련 최적화</td> </tr> </tbody> </table> </li> </ul> <h3 id="기존-유사-연구">기존 유사 연구</h3> <p>| Name | Cite | Context | | — | — | — | | <a href="https://github.com/Stability-AI/generative-models" rel="external nofollow noopener" target="_blank">SV4D</a> | Xie et al. 2024 | * 4D 사물 제작에 특화, 고정된 카메라 각도에서만 동작 가능, 실제 영상의 환경과 괴리감이 큼</p> <ul> <li> <table> <tbody> <tr> <td>이미지 생성 모델의 3D prior + 비디오 생성 모델의 Motion prior</td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td><a href="https://github.com/hi-zhengcheng/vividzoo" rel="external nofollow noopener" target="_blank">Vivid-ZOO</a></td> <td>Li et al. 2024</td> <td>* 4D 사물 제작에 특화, 고정된 카메라 각도에서만 동작 가능, 실제 영상의 환경과 괴리감이 큼</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>LoRA 훈련을 통해 (3D 사물 / 영상간) domain misalignment 을 해결</td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td><a href="https://github.com/CollaborativeVideoDiffusion/CVD" rel="external nofollow noopener" target="_blank">CVD</a></td> <td>Kuang et al. 2024</td> <td>* CameraCtrl 확장, 같은 출발점에서 부터의 다양한 카메라 경로에 대해 생성</td> </tr> </tbody> </table> </li> <li>데이터셋 문제로 인해 좁은 범위의 카메라 경로만 가능</li> <li> <table> <tbody> <tr> <td>SynCamMaster는 시간축에 대한 카메라 경로 편집이 아닌, 복수의 카메라 구도에 대한 생성을 목표로 함</td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td><a href="https://github.com/basilevh/gcd" rel="external nofollow noopener" target="_blank">GCD</a></td> <td>Van Hoorick et al. 2024</td> <td>* 입력 영상과 새로운 카메라 각도에 대해서 새로운 출력 영상을 생성</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>SynCamMaster 는 영상 입력이 아닌 텍스트 입력을 추구함. 해당 목표 달성 이후 Novel View 도 겸사겸사…</td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td><a href="https://github.com/facebookresearch/ViewDiff" rel="external nofollow noopener" target="_blank">ViewDiff</a></td> <td>Hollein et al. 2024</td> <td>* 입력 이미지와 새로운 카메라 각도에 대해서 새로운 출력 이미지를 생성 (유사 시도들과는 다르게, 배경이 가능)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>영상 단위나 전경 레벨에선 문제가 여전함</td> </tr> </tbody> </table> </li> </ul> <h3 id="방법론--multi-view-synchronization-module">방법론 : Multi-View Synchronization Module</h3> <p>DiT 기반 T2V 모델 (주로 CogVideoX) 의 생성 능력을 그대로 살리면서, 복수의 카메라 구도에 대해서 동시에 생성할 수 있게 해주는 어뎁터 모듈을 각 DiT 블록마다 붙인다.</p> <p>입력 : (n : 구도 수, f : 프레임 수, s : 프레임 크기 (w * h))</p> <ul> <li>원 입력 Spatial Feature F_s (n * f * s * d)</li> <li>Camera Params cam (n * 12) <ul> <li>n 개의 카메라 각도중 하나를 global 로 잡고 나머지 normalize 함</li> <li>(p.18 에선 extrinsic 과 plucker embedding 를 사용하는것의 차이에 대해 다루었는데, 큰 차이가 없었다고 결론지었음)</li> </ul> </li> </ul> <p>출력 :</p> <ul> <li>다중 카메라 구도에 최적화된 Spatial Feature F^\bar{v}</li> </ul> <p>방법론: (구도 i 에 대해서 실행)</p> <ol> <li>카메라 인코더 <ol> <li>FC 레이어 (12 채널 —&gt; d 채널) (zero. init)</li> <li>Camera Param 을 d 채널로 바꾸고, (* s) 만큼 repeat</li> <li>이후 F_s 에 element-wise addition</li> </ol> </li> <li>Cross-view Self Attention Layer <ol> <li>모든 구도에서의 Feature 를 다 같이 참고한다</li> <li>원 모델의 Temporal Attention 레이어 weight 로 init</li> <li>이후 projection (Linear+ReLU) (zero init) 를 해서 더한다</li> </ol> </li> </ol> <h3 id="방법론--training-dataset--training-strategy">방법론 : Training Dataset &amp; Training Strategy</h3> <p>총 3종류의 데이터로 구성됨</p> <ol> <li>언리얼 엔진으로 자체 구축된 Multi View Video 데이터 <ol> <li>500개의 3D 배경, 70개의 3D 에셋 (사람, 동물 등) 중에 1~2개 선택 후 애니메이션</li> <li>각 배경당 36개의 카메라, 100개의 프레임 렌더링 <ol> <li>(중앙 기준 3.5~9m, elevation 0~45 에서 랜덤 샘플링)</li> </ol> </li> <li>훈련에서 60%의 확률로 사용</li> </ol> </li> <li>Single View Video 에서 뽁은 Multi View Image 데이터 <ol> <li>RealEstate-10K, DL3DV-10K 는 카메라 경로 정보가 있는 비디오로 구성됨</li> <li>영상에서 프레임들 샘플링해서 사용 <ol> <li>(프레임간 차이가 100을 넘지 않도록 해서 겹치는 구역이 있도록 함)</li> </ol> </li> <li>훈련에서 20%의 확률로 사용</li> <li>Generalization 에 크게 기여</li> </ol> </li> <li>Single View Video (High Quality, 온라인에서 모음) (카메라 정보 없음) <ol> <li>Regularization 으로 사용</li> <li>SynCam 은 고정된 카메라 구도를 상정했기 때문에 카메라 움직임이 크거나 static 한 영상은 배제해야 함 <ol> <li>downsampling 된 영상의 첫 프레임에 SAM 을 통해 객체별 seg.mask 확보</li> <li>CoTracker 를 통해 비디오 내에서 seg.mask 들의 움직임을 계산</li> <li>특정 threshold 미만의 영상들 모두 배제</li> <li>최종적으로 12,000개 영상 선별</li> </ol> </li> <li>훈련에서 20% 확률로 사용</li> </ol> </li> </ol> <p>훈련시, 처음에는 카메라 구도간 차이가 적은 세팅으로 시작해서 점진적으로 크게 가도록</p> <ul> <li>첫 10k : 0 ~ 60도, 10~20k : 30 ~ 90도, 20k~ : 60 ~ 120도</li> </ul> <p>총 50K 훈련, 해상도 (384 * 672), learning rate 1e-4, batch size 32</p> <h3 id="방법론--novel-view-video-synthesis">방법론 : Novel View Video Synthesis</h3> <p>원 세팅 : 입력 텍스트 + 복수의 카메라 구도</p> <p>새로운 세팅 : 입력 텍스트 + 입력 비디오 + 복수의 카메라 구도</p> <p>훈련시, 90% 확률로 첫번째 카메라 구도의 latent 를 입력 비디오의 latent로 교체</p> <p>이후 IP2P 처럼 비디오에 대한 cfg 를 계산해서 적용 (텍스트 : 7.5, 비디오 : 1.8)</p> <h2 id="느낀점">느낀점</h2> <ul> <li>Data Collection 과정이 중요, 어떤 데이터를 어떻게 사용하는지로 논문의 평가가 달라질거라 생각됨</li> <li>거대 비디오 모델 위에 올라타는 형태의 논문은 모델 구성에 한계가 있다고 생각되는데, 그럼 어떤 과정을 거쳐야 독창적인 연구를 제시할 수 있을까?</li> </ul> <h3 id="citation">Citation</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{bai2024syncammaster,
      title={SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints}, 
      author={Jianhong Bai and Menghan Xia and Xintao Wang and Ziyang Yuan and Xiao Fu and Zuozhu Liu and Haoji Hu and Pengfei Wan and Di Zhang},
      year={2024},
      eprint={2412.07760},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.07760}, 
}
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/BirthAndDeathOfRose/">Birth and Death of a Rose</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Weekly-Notes/">LT3SD : Latent Trees for 3D Scene Diffusion</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Animate3D/">Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/TCP/">Towards Controllable and Photorealistic Region-wise Image Manipulation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Swapping_Autoencoder/">Swapping Autoencoder for Deep Image Manipulation</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Patrick Kwon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?v=12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>