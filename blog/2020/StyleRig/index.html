<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="stylerig--rigging-stylegan-for-3d-control-over-portrait-images">StyleRig : Rigging StyleGAN for 3D Control over Portrait Images</h1> <ul> <li>제목 : StyleRig : Rigging StyleGAN for 3D Control over Portrait Images</li> <li>아카이브 ID : <a href="https://arxiv.org/pdf/2004.00121.pdf" rel="external nofollow noopener" target="_blank">2004.00121</a> </li> <li>깃허브 코드 :</li> <li>저자 : A.Tewari, M.Zollhofer, C.Theobalt 등</li> <li>발표 년도 : 2020</li> <li>컨퍼런스 :</li> </ul> <h2 id="overview">Overview</h2> <ul> <li>StyleGAN : $I_w = StyleGAN(w)$ <ul> <li>latent code w : $w \in \mathbb{R}^l$ <ul> <li>$l = 18 \times 512$</li> <li>output of the mapping network in StyleGAN, disentangled</li> <li>18 latent vectors are used for different resolutions.</li> </ul> </li> <li>result portrait image : $I_w \in \mathbb{R}^{3 \times w \times h}$ <ul> <li>$w, h = 1024$</li> </ul> </li> <li>Quality and resolution is good, but no semantic control over output (pose, expression, illumination)</li> </ul> </li> <li>StyleRig : $\hat{w} = RigNet(w, p)$ such that $I_{\hat{w}}=StyleGAN(\hat{w})$ <ul> <li>3DMM parameters : $p=(\alpha, \beta, \delta, \gamma, R, t) \in \mathbb{R}^f$ <ul> <li>Identity : $\alpha \in \mathbb{R}^{80}$</li> <li>Texture : $\beta \in \mathbb{R}^{80}$</li> <li>Expression : $\delta \in \mathbb{R}^{64}$</li> <li>Illumination : $\gamma \in \mathbb{R}^{27}$</li> <li>Rotation : $R \in SO(3)$</li> <li>Translation : $t \in \mathbb{R}^3$</li> </ul> </li> </ul> </li> </ul> <h2 id="network-architecture">Network architecture</h2> <ul> <li>Linear Two-layer perceptron (MLP)</li> <li>2-way cycle consistency losses</li> <li>Differentiable Face Reconstruction</li> </ul> <h3 id="differentiable-face-reconstruction">Differentiable Face Reconstruction</h3> <ul> <li>Maps latent code to 3DMM parameter <ul> <li>$\mathcal{F} : \mathbb{R}^l \to\mathbb{R}^f$</li> <li>$p_w = \mathcal{F}(w)$</li> </ul> </li> <li>3 Layer MLP, ELU activation, self supervised</li> <li>Render Layer : maps 3DMM to rendered face image <ul> <li>$\mathcal{R} : \mathbb{R}^{f} \to \mathbb{R}^{3 \times w \times h}$</li> <li>$S_w = \mathcal{R}(p_w)$</li> </ul> </li> <li>Rendering layer: combination between photometric alignment loss and sparse landmark loss <ul> <li>$\mathcal{L}<em>{render}(I_w,p) = \mathcal{L}</em>{photo}(I_w, p) + \lambda_{land}\mathcal{L}_{land}(I_w, p)$</li> <li>Photometric alignment loss : L2 loss between $I_w$ and $R(p_w)$, inside the binary mask $M$ (region where the face mesh is rendererd <ul> <li>$\mathcal{L}_{photo}(I_w, p) = | M \odot (I_w - \mathcal{R}(p_w))|^2_2$</li> </ul> </li> <li>Sparse landmark loss : L2 loss between 66 automatically computed landmarks on images $I_w$ and $R(p_w)$ <ul> <li>$\mathcal{L}<em>{land}(I_w, p) = | L</em>{I_w} - L_{M}|^2_2$</li> </ul> </li> </ul> </li> <li>Statisicial regularization done on the parameters of the face model</li> <li>Once trained, weights of $\mathcal{F}$ are fixed</li> </ul> <h3 id="rignet-encoder--decoder">RigNet Encoder &amp; Decoder</h3> <ul> <li>Encoder : Maps latent code to lower-dimensional vector $I$ <ul> <li>$I \in \mathbb{R}^{18\times32}$</li> <li>Independent linear encoders for each $i \in {0, …, 17 }$</li> </ul> </li> <li>Decoder : Maps $I$ and 3DMM parameters $p$ to output $\hat{w}$ <ul> <li>Independent linear encoders for each $i \in {0, …, 17 }$</li> <li>Each layer concatenates $I_i$ and $p$ and transforms to $d_i$</li> <li>Final output $\hat{w} = d + w$</li> </ul> </li> </ul> <h2 id="training">Training</h2> <ul> <li>Input : Two separate images &amp; latent code pair; $(v, I_v)$, $(w, I_w)$</li> <li>Output : latent code $\hat{w}$ that has the identity, texture, etc of original image $w$ while having the pose, expression, etc of new image $v$.</li> <li>Loss structure : $\mathcal{L}<em>{total} = \mathcal{L}</em>{rec} + \mathcal{L}<em>{edit} + \mathcal{L}</em>{consist}$</li> <li>Optimization : AdaDelta w/ lr = 0.01</li> </ul> <h3 id="reconstruction-loss">Reconstruction Loss</h3> <ul> <li>L2 loss between the original latent code and the reconstructed latent code</li> <li>Anchors the learned mapping at the right location, prevents degradation of image quality. <ul> <li>Note that DFR ($\mathcal{F}$) is pretrained, thus the semantics of control space are enforced.</li> </ul> </li> <li>$\mathcal{L}_{rec} = | RigNet(w, \mathcal{F}(w))-w|^2_2$</li> </ul> <h3 id="cycle-consistent-per-pixel-loss">Cycle-consistent, Per-pixel loss</h3> <ul> <li>No ground truth (only one picture per person), thus we use cycle-consistent editing &amp; consistency loss.</li> <li>Input $(w, I_w)$ : latent code &amp; image which semantics are transferred to</li> <li>Input $(v, I_v)$ : latent code &amp; image which semantics are transferred from</li> <li>Goal: $I_{\hat{w}}$ should correspond to $I_w$ while is modified according to $p_v$. <ul> <li>$p_v = \mathcal{F}(v)$</li> <li>$\hat{w} = RigNet(w, p_v)$</li> <li>$I_{\hat{w}} = StyleGAN(\hat{w})$</li> <li>$p_{\hat{w}} = \mathcal{F}(\hat{w})$</li> </ul> </li> <li>Enforces that $\hat{w}$ contains the modified parameters of $p_v$, (ex : $p_{\hat{w}}$’s rotation should be similar to $p_v$, etc), while maintaining the original parameters of $p_w$</li> <li>Why not compare $p_v$ and $p_{\hat{w}}$ directly? <ul> <li>Bad practice; perceptual effect of different parameters in image space can be different</li> </ul> </li> </ul> <h4 id="editing-loss">Editing Loss</h4> <ul> <li>$p_{edit}$ : modified version of $p_v$ , which all parameters that needs editing are replaced with $p_{\hat{w}}$’s.</li> <li>$\mathcal{L}<em>{edit} = \mathcal{L}</em>{render}(I_v, p_{edit})$</li> </ul> <h4 id="consistency-loss">Consistency Loss</h4> <ul> <li>$p_{consist}$ : modified version of $p_w$ , which all parameters that needs to be kept are replaced with $p_{\hat{w}}$’s.</li> <li>$\mathcal{L}<em>{consist} = \mathcal{L}</em>{render}(I_w, p_{consist})$</li> </ul> <h3 id="simaese-training">Simaese Training</h3> <ul> <li>Reverse order of $w$ and $v$, and train again</li> <li>Two-way cycle consistency loss</li> </ul> <h2 id="results">Results</h2> <h3 id="style-mixing">Style Mixing</h3> <ul> <li>StyleGAN : Latent codes at different resolutions are copied from source to target to generate new images. <ul> <li> <em>Coarse</em> style : Identity, pose</li> <li> <em>Medium</em> style : Expression, hair structure, illumination</li> <li> <em>Fine</em> style : color scheme</li> </ul> </li> <li>StyleRig : similar, but more control on expression, illumination and pose <ul> <li>Pose (rotation) : Coarse latent code</li> <li>Expression : Coarse + Medium latent code</li> <li>Illumination : Medium + Fine latent code</li> </ul> </li> </ul> <h3 id="interactive-rig-control">Interactive Rig Control</h3> <ul> <li>Explicit semantic control of StyleGAN generated images thru 3DMM parameters <ul> <li>UI that allows interaction with face mesh, which are fed to RigNet</li> </ul> </li> <li>RigNet cannot transfer <em>all</em> modes of parametric control to similar changes in StyleGAN <ul> <li>Ex : in-plane rotation of face mesh is ignored.</li> <li>Ex : many expressions do not transfer well</li> </ul> </li> <li>Why? Bias in StyleGAN trainset <ul> <li>No in-plane rotations, minimum expressions, limited lighting <ul> <li>Because of FFHQ?</li> </ul> </li> </ul> </li> </ul> <h3 id="conditional-image-generation">Conditional Image Generation</h3> <ul> <li>Fix pose / expression /illumination to generate images.</li> <li>Efficient way to generate image</li> </ul> <blockquote> <p>Written with <a href="https://stackedit.io/" rel="external nofollow noopener" target="_blank">StackEdit</a>. </p> </blockquote> </body></html>