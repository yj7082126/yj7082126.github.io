<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="pie-portrait-image-embedding-for-semantic-control">PIE: Portrait Image Embedding for Semantic Control</h1> <ul> <li>제목 : PIE : Portrait Image Embedding with Semantic Control</li> <li>아카이브 ID : <a href="https://arxiv.org/pdf/2009.09485.pdf" rel="external nofollow noopener" target="_blank">2009.09485</a> </li> <li>깃허브 코드 :</li> <li>저자 : A.Tewari, M.Zollhofer, C.Theobalt 등</li> <li>발표 년도 : 2020</li> <li>컨퍼런스 :</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li>주제 : 얼굴 이미지 편집</li> <li>선행조건 : Parametrized control with semantic meaning</li> <li>이전시도 : Synthetic StyleGAN image 한정</li> <li>해결책 : Real Image 대상의 Pose / Expression / Illumination 편집</li> <li>방법 1 : StyleRig (pretrained NN) <ul> <li>3DMM –&gt; StyleRig –&gt; StyleGAN latent space</li> <li>non-linear optimization problem</li> </ul> </li> <li>방법 2 : Identity preservation energy term ($E(w)$) <ul> <li>중요 1 : $StyleGAN(w)$ 의 high fidelity 보장</li> <li>중요 2 : Expression 등의 편집을 가능케 함</li> <li>중요 3 : Identity 등의 보존</li> <li>Non-linear optimization : Network weight를 추가로 배우는 것이 아니며, pretrained 된 네트워크를 기반으로 구성되므로 ground truth image 가 필요없다.</li> </ul> </li> </ul> <h2 id="notation">Notation</h2> <ul> <li>$I$ : 진짜 얼굴 이미지 (입력 이미지)</li> <li>$w$ : I 의 편집을 가능케 하는, StyleGAN latent embedding (출력 코드) <ul> <li>$v$ : 다른 general StyleGAN embedding</li> </ul> </li> <li>$\theta$ : 3DMM parameter <ul> <li>$\theta =(\phi, \rho, \alpha, \delta, \beta, \gamma ) \in \mathbb{R}^{257}$ <ul> <li>$(\phi, \rho) \in \mathbb{R}^{6}$ : rotation / translation</li> <li>$\alpha \in \mathbb{R}^{80}$ : identity</li> <li>$\beta \in \mathbb{R}^{64}$ : expression</li> <li>$\delta \in \mathbb{R}^{80}$ : texture</li> <li>$\gamma \in \mathbb{R}^{27}$ : illumination</li> </ul> </li> <li>$\tau \in {\phi, \beta, \gamma }$ : editable semantic variables <ul> <li>$\theta^{\tau}$ : extraction of $\tau$ component</li> <li>$\theta’ = [\theta^{\bar{\tau}}_1, \theta^{\tau}_2]$ : combine $\bar{\tau}$ components of $\theta_1$ with $\tau$ components of $\theta_2$</li> <li>$\theta(v), \theta_v$ : StyleGAN 이미지 $I(v)$ 에서 추출된 3DMM parameter <ul> <li>Pretrain 된 Model-based Face AutoEncoder (MoFA) 네트워크를 사용하여 StyleGAN을 통해 생성된 이미지에서 3DMM parameter를 추출한다.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related work</h2> <h3 id="1-person-specific-video-editing-model-based">1. Person-specific Video Editing (Model based)</h3> <p>단일 인물의 많은 양의 사진을 필요로 하는 경우; 긴 길이의 비디오가 필요하며, 특정 인물의 단일 이미지로 사용될 수 없음</p> <ul> <li>Source Video &amp; Target Video <ul> <li> <a href="https://arxiv.org/pdf/1904.12356.pdf" rel="external nofollow noopener" target="_blank">Face2Face (Thies, 2016~2019)</a> <ul> <li>(Source Video Expression) + (Target Video)</li> <li>Target의 비디오 필요, pose / illumination 편집 불가</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/1805.11714.pdf" rel="external nofollow noopener" target="_blank">Deep Video Portraits (Kim, 2018)</a> <ul> <li>Expression / Pose 편집</li> <li>Speaking Style 편집 불가</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/1905.02518.pdf" rel="external nofollow noopener" target="_blank">Neural Style-Preserving Visual Dubbing (Kim, 2019)</a> <ul> <li>Speaking Style 을 보존하면서 Expression / Pose 편집 <ul> <li>Style Translation Network</li> </ul> </li> </ul> </li> </ul> </li> <li>Source Video &amp; Target Face <ul> <li> <a href="https://arxiv.org/pdf/1808.05174.pdf" rel="external nofollow noopener" target="_blank">Recycle-GAN (Bansal, 2018)</a> <ul> <li>Speaking Style 을 보존하면서 Expression / Pose 편집</li> <li>Spatio-temporal video domain의 recycle loss</li> </ul> </li> </ul> </li> </ul> <h3 id="3-few-shot-editing">3. Few-shot Editing</h3> <p>단일 인물의 적은 양의 사진을 필요로 하는 경우;</p> <ul> <li> <a href="https://arxiv.org/pdf/1807.10550.pdf" rel="external nofollow noopener" target="_blank">X2Face (Wiles, 2018)</a> <ul> <li>Encoder-decoder 구조, 정면 얼굴 및 음성을 입력으로 받는다.</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/1910.12713.pdf" rel="external nofollow noopener" target="_blank">Few-shot Video-to-Video (Wang, 2019)</a> <ul> <li>스케치 영상을 사진으로 만드는 네트워크 + attention mechanism</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/1905.08233.pdf" rel="external nofollow noopener" target="_blank">Realistic Neural Talking Head Models (Zakharov, 2019)</a> <ul> <li> <ol> <li>Generator (Landmark –&gt; Image)</li> </ol> </li> <li> <ol> <li>Embedding (Representation for generator conditioning)</li> </ol> </li> <li> <ol> <li>Discriminator</li> </ol> </li> </ul> </li> </ul> <h3 id="4-single-shot-editing">4. Single-shot Editing</h3> <ul> <li> <a href="http://www.hao-li.com/publications/papers/siggraphAsia2018PAGAN.pdf" rel="external nofollow noopener" target="_blank">paGAN (Nagano, 2018)</a> <ul> <li>Personalized avatar from a single image</li> <li>Does not synthesize photo-realistic hair</li> </ul> </li> <li> <a href="http://cs.tau.ac.il/~averbuch1/portraitslife/elor2017_bringingPortraits.pdf" rel="external nofollow noopener" target="_blank">Bringing Portraits to Life (Averbuch-Elor, 2017)</a> <ul> <li>Perform 2D Warp to face image, to animate expression and pose</li> </ul> </li> <li> <a href="http://eprints.whiterose.ac.uk/138578/1/wgGAN.pdf" rel="external nofollow noopener" target="_blank">Warp-Guided GAN (Geng, 2018)</a> <ul> <li>Deep Generative models for higher quality</li> <li>Spatial Motion Field</li> <li>Network 1(Skin Detail)</li> <li>Network 2(Mouth Interior)</li> </ul> </li> <li> <a href="https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation.pdf" rel="external nofollow noopener" target="_blank">First Order Model (Siarohin, 2019)</a> <ul> <li>Detect keypoints &amp; generate warp field out of it.</li> <li>Can work both for face and full-body image</li> </ul> </li> </ul> <h3 id="5-image-editing-with-stylegan">5. Image Editing with StyleGAN</h3> <ul> <li> <a href="https://arxiv.org/pdf/1911.11544.pdf" rel="external nofollow noopener" target="_blank">Image2StyleGAN++</a> <ul> <li>Embed real image into StyleGAN latent space, with high fidelity</li> </ul> </li> <li><a href="https://arxiv.org/pdf/1907.10786.pdf" rel="external nofollow noopener" target="_blank">InterFaceGAN</a></li> <li><a href="https://arxiv.org/pdf/2008.02401.pdf" rel="external nofollow noopener" target="_blank">StyleFlow</a></li> <li><a href="https://arxiv.org/pdf/2004.00049.pdf" rel="external nofollow noopener" target="_blank">In-domain GAN Inversion</a></li> <li><a href="https://arxiv.org/pdf/2004.00121.pdf" rel="external nofollow noopener" target="_blank">StyleRig</a></li> </ul> <h2 id="semantic-editing-of-real-facial-images">Semantic Editing of Real Facial Images</h2> <ul> <li>StyleRig의 한계 : 진짜 얼굴 이미지가 아닌, StyleGAN 합성 이미지만 사용 가능. <ul> <li>해결책 : 얼굴 이미지를 StyleGAN latent embedding으로 변환 <ul> <li>가장 말이 되는 방향으로 찾는 점이 중요</li> </ul> </li> </ul> </li> </ul> <p>$E(w) = E_{synth}(w) + E_{identity}(w) + E_{edit}(w) + E_{invariance}(w) + E_{recognition}(w)$</p> <h3 id="high-fidelity-image-synthesis-e_synth">High-Fidelity Image Synthesis (E_synth)</h3> <p>$E_{synth}(w) = \lambda_{l_2}|I-I_w|^2_2+\lambda_{p}|\Phi(I)-\Phi(I_w) |^2_2$</p> <ul> <li>latent code $w$ 를 바탕으로 구성한 StyleGAN 이미지 $I_w$ 가 원본 이미지 $I$와 유사하도록 한다.</li> <li>Term 1 : 두 이미지 간 L2 Distance</li> <li>Term 2 : 두 이미지 간 Perceptual Distance <ul> <li>$\Phi(\cdot)$ : VGG-16 layer 에서 얻어지는 feature들</li> </ul> </li> <li>이 공식 만 가지고 원본 이미지와 유사한 이미지를 생성할 수 있지만, 편집을 하는 목적에는 suboptimal 하다.</li> </ul> <h3 id="face-image-editing--identity-preservation-e_identity">Face Image Editing : Identity Preservation (E_identity)</h3> <p>$E_{identity}(w) = \lambda_{identity}|w-RigNet(w, \theta^\tau_w) |^2_2$</p> <ul> <li>RigNet을 통하여 latent code $w$를 재구성 하여도 원래 $w$ 와 똑같아야 한다.</li> </ul> <h3 id="face-image-editing--editing-property-e_edit">Face Image Editing : Editing Property (E_edit)</h3> <p>$\forall v : I_v=I_{([\theta^{\bar{\tau}}_v,\theta^{\tau}_{RigNet(w, \theta^{\tau}_v)}])}$ : Edit Property $(\theta^{\tau}_{v} \approx RigNet(w, \theta^{\tau}_{v}))$</p> <ul> <li>$\theta_v$의 non-semantic component와, RigNet을 통하여 재구성된 $\theta_v$의 semantic component를 합친 결과로 이미지를 구성시, 원래 StyleGAN 이미지 $I_v$ 와 같아야 한다. <ul> <li>물론 StyleGAN 이미지와 mesh rendered 이미지가 같을 순 없지만, 그 차이를 최소화 해야한다.</li> </ul> </li> </ul> <p>$\ell(I’, \theta) = \lambda_{photometric}|I’ - I_\theta|^2_{face} + \lambda_{landmark}|\mathcal{L}_{I’} - \mathcal{L}_\theta |^2_F$</p> <ul> <li>Term 1 : Photometric loss : $I’$ 와 3DMM parameter $\theta$를 토대로 렌더링한 $I_\theta$ 사이, 얼굴 부분에만 한정 지은 L2 Distance</li> <li>Term 2 : Landmark loss : 두 이미지의 랜드마크 ($\mathcal{L}_I \in \mathbb{R}^{66\times2}$) 간 Frobenius norm</li> </ul> <p>$E_{edit}(w) = \lambda_{edit}\mathbb{E}_v[\ell(I_v, [\theta^{\bar{\tau}}_v, \theta^{\tau}_{RigNet(w, \theta^{\tau}_v)}])]$</p> <h3 id="face-image-editing--editing-property-e_invariance">Face Image Editing : Editing Property (E_invariance)</h3> <p>$\forall v : I=I_{([\theta^{\bar{\tau}}_{RigNet(w, \theta^{\tau}_v)}, \theta^{\tau}_I, ])}$ : Invariance Property $(\theta^{\bar{\tau}}_w \approx RigNet(w, \theta^{\tau}_v)$</p> <ul> <li>RigNet을 통하여 재구성된 $\theta_w$의 non-semantic component와, $\theta_I$의 semantic component를 합친 결과로 이미지를 구성시, 원래 이미지 $I$와 같아야 한다.</li> </ul> <p>$E_{invariance}(w) = \lambda_{invariance}\mathbb{E}_v[\ell(I, [\theta^{\bar{\tau}}_{RigNet(w, \theta^{\tau}_v)}, \theta^{\tau}_I])]$</p> <h3 id="face-recognition">Face Recognition</h3> <ul> <li>VGG-Face에서 사람이 얼굴 인지와 관련된 feature들을 사용하여 Face Recognition Consistency를 유지한다. <ul> <li>$\Psi(I)$ : VGG-Face를 통해 얻어진 feature들</li> </ul> </li> </ul> <p>$\ell_{recog}(I’, v) = | \Psi(I’) - \Psi(I_v)|^2_F$ $E_{recognition} = \lambda_{r_w}\ell_{recog}(I, w) + \lambda_{r_{\hat{w}}}\mathbb{E}_{V}[\ell_{recog}(I, RigNet(w, \theta^{\tau}_{v})]$</p> <h3 id="optimization">Optimization</h3> <ul> <li>$E(w)$ 에는 StyleGAN, MoFA, $\Psi$, $\Phi$ 등의 non-linear function등이 pretrained neural network의 형태로 사용된다.</li> <li>Tensorflow 기반의 AdaDelta optimization을 사용하며, 각 Iteration 마다 다른 v값을 사용한다.</li> <li>Hierarchical Optimization: <ul> <li> <table> <tbody> <tr> <td>StyleGAN 의 Latent Space 은 Hierarchical order로, 여러개의 $</td> <td>W</td> <td>= 512$ 가 쌓여서 $</td> <td>W^{+}</td> <td>= 18 \times 512$를 이루는 형태.</td> </tr> </tbody> </table> </li> <li>$W^{+}$는 이미지를 표현하는데 최상의 latent space이지만, 직접적으로 $W^{+}$ 에 optimization을 실행하는 것은 좋은 퀄러티의 결과물을 생산하지 않는다. <ul> <li>해당 결과물은 StyleGAN의 prior distribution과 멀리 떨어진 경우가 많기 때문</li> </ul> </li> <li>따라서, $W$ 상에서 optimization을 실행한 후 (2000 step), $W^{+}$ 로 결과물을 변환시킨후 다시 optimization을 실행한다 (1000 step) <ul> <li>이리 함으로서 coarse-to-fine 구조를 확립시킨다. <blockquote> <p>Written with <a href="https://stackedit.io/" rel="external nofollow noopener" target="_blank">StackEdit</a>. </p> </blockquote> </li> </ul> </li> </ul> </li> </ul> </body></html>