<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="stylegan--a-style-based-generator-architecture-for-generative-adversarial-networks">StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks</h1> <ul> <li>제목 : A Style-Based Generator Architecture for Generative Adversarial Networks</li> <li>아카이브 ID : <a href="https://arxiv.org/pdf/1812.04948.pdf" rel="external nofollow noopener" target="_blank">1812.04948</a> </li> <li>깃허브 코드 : https://github.com/NVlabs/stylegan</li> <li>저자 : T.Karras, S.Laine, T.Aila 등</li> <li>발표 년도 : 2018</li> </ul> <h3 id="overview">Overview</h3> <ul> <li>Problem : While generative adversarial networks (GAN) based image generation has met rapid improvement in resolution and quality, the process remains unknown. <ul> <li> <strong>Properties of latent space</strong>, and other aspects of the image synthesis process is still <strong>not fully understood</strong>.</li> </ul> </li> <li>Solution : re-design GAN to expose novel-ways to control the process. <ul> <li> <strong>Starts from constant input</strong>, and for each passing convolution layer, <strong>adjust the “style” of image.</strong> <ul> <li>Leads to automatic, <strong>unsupervised separation of high-level attributes</strong> (pose, identity) from stochastic variation (freckles, etc) in generated images.</li> <li>No modification of discriminator or loss f.</li> </ul> </li> <li>Embed the input latent code into a <strong>intermediate latent space</strong> <ul> <li>Free input latent space from being entangled with training data.</li> <li>Metrics : perceptual path length and linear separability.</li> </ul> </li> </ul> </li> <li>지금까지 존재해 온 생성적 적대 모델 (GAN)을 사용한 얼굴 이미지 합성 기술들은 해상도와 전반적인 질에 있어서 가파른 발전을 보여 왔지만, 그 합성의 세부 과정들 및 잠재 공간 (Latent Space) 은 여전히 미지의 영역으로 남겨져 왔습니다. <ul> <li>또한, 이로 인해 얼굴 합성 시 성별이나 연령 등, 사진의 세부요소</li> </ul> </li> </ul> <h3 id="style-based-generator">Style-based generator</h3> <p><img src="/assets/images/stylegan-1.png" alt="STYLEGAN model structure"></p> <ul> <li>Mapping Network: <ul> <li>Instead of directly injecting the latent code \(\mathbb{z} \in \mathcal{Z}\), the paper maps the latent code to intermediate latent space \(f : \mathcal{Z} \rightarrow \mathcal{W}\) \((\mathbb{w} \in \mathcal{W})\)</li> <li>This can be viewed as a way to draw samples for each style from a learned distribution.</li> <li>The dimensionality of \(\mathcal{Z}\) and \(\mathcal{W}\) are the same (512)</li> <li>Mapping is implemented with 8 fully-connected layers.</li> </ul> </li> <li>Synthesis network (Generator): <ul> <li>The mapped \(\mathbb{w}\) is then shaped to styles \(y = (y_{s}, y_{b})\) for adaptive instance normalization (AdaIN) which is applied to each layer of the generator.</li> <li>This can be viewed as a way to generate a novel image based on a collection of styles.</li> <li> \[AdaIN(x_{i}, y) = y_{s,i}\frac{x_{i}-\mu(x_{i})}{\sigma({x_{i}})} + y_{b,i}\] </li> <li>Each feature map is normalized separately, and then scaled/biased from style y. <ul> <li>Thanks to the normalization, the style statistics does not depend on the original statistics, and modifies the relative importance of features. <ul> <li>Thus, modifying a specific subset of the styles can be expected to affect only certain aspects of the image</li> </ul> </li> </ul> </li> </ul> </li> <li>Style Mixing <ul> <li>Mixing regularization : given percentage of images (90%) are generated using two random latent codes instead of one during training.</li> <li>How? Style-mixing : At a randomly selected point in the generator, switch the latent code to another.</li> <li>Prevents the network from assuming that adjacent styles are correlated.</li> </ul> </li> <li>Stochastic variation <ul> <li>Stochastic factors (ex : hair placement, freckles) can be randomized</li> <li>Previous : network had to invent way to generate spatially-varying pseudorandom numbers; consumes network capacity, and not always successful.</li> <li>Solution : per-pixel noise after each convolution.</li> <li>Noise only affects inconsequential stochastic variation <ul> <li>Spatially invariant statistics (gram matrix, channel-wise mean/variance, etc) reliably encode the style of an image, while variant features encode specific instances.</li> <li>Since style features are scaled/biased with same values, global effects can be controlled coherently.</li> <li>The noise is applied per-pixel, thus suited for controlling stochastic variation.</li> <li>If the generator attempted to use noise to control global effects, the decision would be spatially inconsistent and thus penalized by the discriminator.</li> </ul> </li> </ul> </li> </ul> <h3 id="disentanglement-studies">Disentanglement studies</h3> <ul> <li>Previous : given a training set with some combination is missing (ex : long haired males), the input latent space would have a missing space, forcing the latent space to be nonlinear and prevents from full disentanglement.</li> <li>Solution : the intermediate latent space is not fixed in the same manner, and instead induced by the learned piecewise continuous mapping. <ul> <li>Since it should be easier for the generator to generate images based on a disentangled representation rather than a entangled one, the generator forces the variational factors to be more linear.</li> <li>Previous metrics proposed for disentanglement requires an encoder network which is unsuitable for STYLEGAN architecture, two new ways are introduced</li> </ul> </li> <li>Perceptual path length <ul> <li>Interpolation of latent space vectors can show non-linear changes in the image (ex : new feature appearing out of nowhere from linear interpolation), which indicates entanglement of latent space.</li> <li>Measure the level of change during interpolation in latent space: <ul> <li>Less “curved” (i.e more linear) latent space should show more smoother transition.</li> </ul> </li> </ul> </li> <li>Perceptual path length : weighted difference between VGG16 embeddings. <ul> <li>Given \(z_1, z_2\) from input latent space, \(t \sim Uniform(0,1)\), \(G\) as the generator, and \(d(.,.)\) as the perceptual difference between two generated images, \(l_{Z} = \mathbb{E}[\frac{1}{\epsilon^2}d(G(slerp(z_1,z_2;t)), G(slerp(z_1,z_2;t+\epsilon)))]\) <ul> <li>Total perceptual length between path = sum of the perceptual differences over each segment.</li> <li>Epsilon</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Written with <a href="https://stackedit.io/" rel="external nofollow noopener" target="_blank">StackEdit</a>. </p> </blockquote> </body></html>