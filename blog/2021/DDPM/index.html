<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="diffusion-model-이란">Diffusion Model 이란</h2> <blockquote> <p>데이터셋의 이미지들에 작은 노이즈를 주입하는 과정들로 구성된 “정방향 프로세스”가 있을 때, 해당 프로세스의 반대인 “역방향 프로세스”를 배워, 노이즈로부터 데이터셋 분포에 포함된 샘플을 생성하는 모델. </p> </blockquote> <p>예를 들어, CelebA 데이터셋에서 뽑은 얼굴 $x_0$가 있다고 가정하겠습니다. CelebA 의 데이터셋 분포를 $q(x)$ 라고 하면, $x_0 \sim q(x)$ 라고 할 수 있겠죠.</p> <p>이 $x_0$ 에 극소량의 노이즈 (가우시안으로 고정)을 추가하는 방법을 무한히 반복하면, T 시점의 데이터 $x_T$ 는 원래 얼굴을 알아볼 수 없는, 가우시안 분포 $\mathcal{N}(0, \mathbf{I})$ 에 속해있을 수 있습니다.</p> <p>이론상은 이 노이즈 추가를 무한히 반복해야겠지만, 실제로는 노이즈 삽입을 250~1,000회 이상 실행하는 것으로 기존 데이터를 노이즈로 바꿀 수 있습니다.</p> <p>Diffusion 모델은 바로 이 노이즈 삽입 과정의 역을, 즉 이미지와 노이즈가 섞인 데이터에서 극소량의 노이즈를 제거하는 과정을 배움으로써, 가우시안 분포에서 데이터셋 분포로 바꾸는 작업을 수행할 수 있게 됩니다.</p> <h2 id="forward-diffusion-process-정과정">Forward Diffusion Process (정과정)</h2> <p>정과정 에서는 최초 데이터 $x_0$ 에서부터 노이즈를 더하는 방식으로  $x_1, x_2, …, x_T$ 의 샘플들을 생성하며,  이 과정에 각 스텝은 변수 ${\beta_t \in (0,1)}^T_{t=1}$ 로 통제됩니다. 후반부의 스텝으로 갈 수록 안전하게 큰 노이즈를 추가할 수 있기 때문에, $\beta$를 점진적으로 증가하는 구조로 만들 수 있습니다. ($\beta_1 &lt; \beta_2 &lt; … &lt; \beta_T$)</p> <p>또한, 정과정은 마르코브 체인의 형태로 구성됩니다; $x_t$ 시점의 분포는, $x_{t-1}$ 에 의해서만 전제됩니다.</p> <blockquote> <p>\(q(x_{t} | x_{t-1}) \approx \mathcal{N}(x_{t}; \sqrt{1 - \beta_t}x_{t-1}, \beta_t\mathbf{I}) \\ q(x_{1:T}|x_0) \approx \prod^{T}_{t=1} q(x_t|x_{t-1})\) (평균값을 줄이고, 표준편차를 늘리면서 가우시안 정규분포에 가까워진다.)</p> </blockquote> <p>이 조건부 분포에서, VAE에서 쓴 reparameterization trick을 쓰면 $x_{t-1}$를 통해 $x_t$를 구할 수 있습니다. ($x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}z$)</p> <p>그리고 마르코브 체인에 따라 분포들을 합쳐가다 보면, $x_0$과 $t$가 있으면 를 $x_t$구할 수 있습니다. 편의를 위해 $\alpha_t = 1 - \beta_t$, $\bar{\alpha_t} = \prod^{t}_{s=1}\alpha_s$ 로 쓰면</p> <blockquote> <p>\(\begin{align*} x_t =&amp; \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z \\ =&amp; \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t}\alpha_{t-1}}z \\ =&amp; \text{ } ... \\ =&amp; \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}z \\ \end{align*}\) \(q(x_t|x_0) \approx \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1 - \bar{\alpha_t})\mathbf{I})\) 으로 식을 전개할 수 있음을 확인할 수 있습니다.</p> </blockquote> <h2 id="reverse-diffusion-process-역과정">Reverse Diffusion Process (역과정)</h2> <p>만약 저희가 $q(x_{t-1}|x_t)$를 알 수 있으면, 가우시안 노이즈 $q(x_T)$에서 출발해, 데이터셋에 맞는 이미지를 생산할 수 있습니다. 그걸 알 수 없기 때문에 딥러닝으로 해당 분포를 근사해야만 합니다. 이 $q(x_{t-1}|x_t)$ 의 “근사 함수” 를 $p_{\theta}(x_{t-1}|x_t)$로 정의합니다. ($\theta$ : 파라미터 값)</p> <h2 id="training-loss">Training Loss</h2> <h2 id="ddpm-parameterization">DDPM Parameterization</h2> <h2 id="training-and-sampling">Training and Sampling</h2> <h2 id="case-of-l_0">Case of $L_0$</h2> <h2 id="참고문헌">참고문헌</h2> <p><img src="https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html" alt="Lilian Weng : What are Diffusion Models?"></p> </body></html>