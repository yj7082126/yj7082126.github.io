<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="introduction">Introduction</h3> <ul> <li>By optimizing volumetric scene function using sparse set of input views, we can synthesize novel views of complex scenes.</li> <li>Input : Scenes consisted of continuous <em>spatial locations</em> $(x, y, z)$ and <em>viewing directions</em> $(\theta, \phi)$</li> <li>Output : <em>Volume density</em> and <em>view-dependent color</em> at spatial location. <ul> <li>Density : differential opacity controlling the amount of radiance accumulated by a ray passing thru position.</li> </ul> </li> <li>Model : MLP, without convolutional layers. <ul> <li>Because basic implementation does not converge to sufficient representation, we use the following; <ul> <li>We transform the input coordinates with positional encoding, to represent higher frequency functions.</li> <li>We propose hierarchical sampling procedure to reduce number of queries.</li> </ul> </li> </ul> </li> <li>Using traditional volume rendering techniques, we can project the output into synthesized images.</li> </ul> <h3 id="related-works--neural-3d-shape-representations">Related Works : Neural 3D Shape representations.</h3> <ul> <li>Implicit representation of 3D shapes as level sets <ul> <li>Method #1 : (x, y, z) coords to signed distance functions. <ul> <li>Curless, B : <a href="https://www.semanticscholar.org/paper/A-volumetric-method-for-building-complex-models-Curless-Levoy/07f254f33e4c99d4c443d9a5d9221f1e9af6c106" rel="external nofollow noopener" target="_blank">A volumetric method for building complex models from range images</a> </li> <li>Jiang, C : <a href="https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/projects/local_implicit_grid/README.md" rel="external nofollow noopener" target="_blank">Local implicit grid representations for 3d scenes</a> </li> <li>Park, J : <a href="https://github.com/facebookresearch/DeepSDF" rel="external nofollow noopener" target="_blank">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</a> </li> </ul> </li> <li>Method #2 : (x, y, z) coords to occupancy fields. <ul> <li>Genova, K : <a href="https://github.com/google/ldif" rel="external nofollow noopener" target="_blank">Local Deep Implicit Functions for 3D Shape</a> </li> <li>Mescheder, L : <a href="https://github.com/autonomousvision/occupancy_networks" rel="external nofollow noopener" target="_blank">Occupancy networks: Learning 3D reconstruction in function space</a> </li> </ul> </li> <li>Limited by requirement of ground truth 3D geometry (ex : <a href="https://shapenet.org/" rel="external nofollow noopener" target="_blank">ShapeNet</a>)</li> <li>Recent : relax this requirement by functions that allow neural implicit shape representations to be optimized through 2D images. <ul> <li>Niemeyer, M : <a href="https://github.com/autonomousvision/differentiable_volumetric_rendering" rel="external nofollow noopener" target="_blank">Differentiable volumetric rendering</a> </li> <li>Sitzmann, V : <a href="https://github.com/vsitzmann/scene-representation-networks" rel="external nofollow noopener" target="_blank">Scene representation networks</a> </li> <li>Still limited to simple shapes.</li> </ul> </li> </ul> </li> </ul> <h3 id="related-works--view-synthesis-and-image-based-rendering">Related Works : View synthesis and image-based rendering.</h3> <ul> <li>Light field sample interpolation <ul> <li>Levoy, M : <a href="https://www.cs.princeton.edu/courses/archive/fall12/cos526/papers/levoy06.pdf" rel="external nofollow noopener" target="_blank">Light field rendering</a> </li> </ul> </li> </ul> <h3 id="nerf-representation">NeRF Representation</h3> <ul> <li>Input : <ul> <li>3D Location $\mathbf{x} = (x, y, z)$</li> <li>2D Viewing direction $\mathbf{d} = (\theta, \phi)$</li> </ul> </li> <li>Output : <ul> <li>Emitted color $\mathbf{c} = (r, g, b)$</li> <li>Volume density $\sigma$</li> </ul> </li> <li>Network <ul> <li>$F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$</li> <li>In order to make the representation <strong>multiview consistent</strong>, restrict the network such that <strong>volume density $\sigma$ is only predicted by location $x$</strong>, regardless of direction. <ul> <li>$\mathbf{x} \rightarrow MLP(*8, 256) \rightarrow (\sigma, f.v)$</li> <li>$Concat(f.v, \mathbf{d}) \rightarrow MLP(*1, 128) \rightarrow (\mathbf{c})$</li> </ul> </li> </ul> </li> </ul> <h3 id="volume-rendering-with-radiance-field">Volume Rendering with Radiance Field</h3> <blockquote> <p>Written with <a href="https://stackedit.io/" rel="external nofollow noopener" target="_blank">StackEdit</a>. </p> </blockquote> </body></html>