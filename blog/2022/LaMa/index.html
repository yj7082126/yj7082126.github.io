<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>LaMa - Large Mask Inpainting | Patrick Kwon (권용재)</title> <meta name="author" content="Patrick Kwon"> <meta name="description" content="Academic Portfolio webpage for Patrick Kwon (권용재), Deep learning Researcher at Naver Webtoon "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yj7082126.github.io/blog/2022/LaMa/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Patrick Kwon (권용재)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">LaMa - Large Mask Inpainting</h1> <p class="post-meta">January 30, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/image-inpainting"> <i class="fas fa-hashtag fa-sm"></i> image inpainting</a>     ·   <a href="/blog/category/blog"> <i class="fas fa-tag fa-sm"></i> blog</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="lama-resolution-robust-large-mask-inpainting-with-fourier-convolutions">LaMa (Resolution-robust Large Mask Inpainting with Fourier Convolutions)</h2> <table> <thead> <tr> <th>이름</th> <th><strong>LaMa : Resolution-robust Large Mask Inpainting with Fourier Convolutions</strong></th> </tr> </thead> <tbody> <tr> <td>저자</td> <td>Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky</td> </tr> <tr> <td>토픽</td> <td>Image Inpainting</td> </tr> <tr> <td>년도</td> <td>2021.09</td> </tr> <tr> <td>학회</td> <td> </td> </tr> <tr> <td>링크</td> <td> <a href="https://arxiv.org/abs/2109.07161" rel="external nofollow noopener" target="_blank">[논문]</a>, <a href="https://github.com/saic-mdal/lama" rel="external nofollow noopener" target="_blank">[코드]</a> </td> </tr> </tbody> </table> <p><img src="/assets/images/lama/lama-1.gif" alt="Intro"></p> <h3 id="intro">Intro</h3> <p><strong>이미지 인페인팅 (주어진 이미지와 (유저가 입력한) ‘마스크’가 있을때, 마스크로 가려진 부분들을 자연스럽게 유추하는 문제)</strong> 는 이미지 생성뿐만이 아닌, <strong>거시적인 이미지 구조를 ‘이해’하는</strong> 것을 요구합니다.</p> <p>현재 GAN을 통한 이미지 생성이 실제와 분간이 되지 않을 정도록 뛰어난 성능을 보임에도 불구하고, 아직 이미지 인페인팅 문제는 이미지의 대부분을 덮는 마스크나 복잡한 구조의 마스크, 고해상도의 이미지 등의 상황에 대해 제대로 대처를 하지 못하고 있으며, 본 논문의 저자들은 이를 <strong>small receptive field의 문제</strong> 라고 보고 있습니다.</p> <p>이를 해결하기 위해, 저자들은 LaMa(Large Mask Inpainting) 모델을 소개합니다. LaMa는 <strong>Fast Fourier Convolution</strong> 을 기반으로 하여 모델의 초기 단계에서 부터 이미지 전체를 덮을수 있는 Receptive Field를 가지고 있으며, loss function 도 <strong>High Receptive Field를 기반으로한 Perceptual loss를</strong> 사용하여 이미지의 거시적인 구조를 수월하게 이해할 수 있습니다. Fast Fourier Convolution의 특성상, <strong>고해상도의 이미지에 대해서도 문제없이 적응이</strong> 가능합니다. 또한 훈련 단계에서 크고 복잡한 모양의 마스크들을 사용해서 해당 마스크들에 대한 저항력을 기릅니다.</p> <p>저자들은 연구를 통해, 256해상도에서만 훈련시킨 본 모델로도 고해상도의 이미지와 큰 마스크들에 대해서 자연스럽게 이미지 인페인팅을 수행할 수 있음을 보여줍니다.</p> <hr> <h3 id="main-reference-works">Main Reference Works</h3> <ul> <li>[4] : <a href="https://github.com/pkumivision/FFC" rel="external nofollow noopener" target="_blank">Fast fourier convolution</a> (<a href="https://arxiv.org/abs/2010.04257" rel="external nofollow noopener" target="_blank">논문</a>)</li> <li>[29] : <a href="https://arxiv.org/abs/1701.04128" rel="external nofollow noopener" target="_blank">Understanding the effective receptive field in deep convolutional neural networks</a> </li> </ul> <hr> <h3 id="method">Method</h3> <p>이미지가 x, 마스크가 m일때, 모델 f(변수: $\theta$)는 마스크로 덮혀진 이미지와 마스크를 입력으로 받아, 마스크 부분이 인페인팅된 이미지를 받습니다.</p> \[\hat{x}=f_{\theta}(\text{stack}(x \odot m,m))\] <table> <thead> <tr> <th><img src="/assets/images/lama/lama-2.png" alt="FFC구조"></th> </tr> </thead> <tbody> <tr> <td>FFC 모델의 전반적인 구조. Local (미시적 요소) / Global (거시적 요소) 로 나뉜 feature들 중에서 Local은 고전적인 convolution을, Global은 Fourier Transformation을 통해 얻은 주파수에서의 convolution을 통해 모델의 이미지 전체에 대한 이해를 돕습니다.</td> </tr> </tbody> </table> <h3 id="fast-fourier-convolution-ffc">Fast Fourier Convolution (FFC)</h3> <p>ResNet과 같은 기존 convolution 기반 모델들은, <strong>작은 사이즈의 커널 (3*3 사이즈)</strong> 때문에 유효한 receptive field가 초반에 작아서 이미지 전체를 덮지 못하며, 커지는 속도도 느립니다. 따라서 모델 내부 대다수의 레이어들은 거시적인 구조에 대한 정보를 담지 못하고, 인페인팅시 썩 자연스럽지 못한 결과가 나옵니다. 특히 면적이 큰 마스크들의 경우, <strong>receptive field 전체가 마스크 안에 있게 되는 불상사</strong>가 발생하여 인페인팅이 불가능한 경우도 발생합니다.</p> <p>이를 막기 위해 초기 단계에서 거시적인 구조에 대한 정보를 담을 수 있는 레이어가 필요하고, 이를 위해 <strong>FFC (Fast Fourier Convolution)이</strong> 필요합니다.</p> <ul> <li>FFC는 채널 단위 <strong>Fast Fourier Transform (FFT)</strong> 를 사용하며, 이를 통해 이미지 전체를 덮을 수 있는 receptive field를 가집니다.</li> <li>FFC는 두 가지 갈래의 연산을 수행하는데, <em>local branch</em>는 미시적인 구조를 위한 고전적인(?) convolutional layer를, <em>global branch</em>는 거시적인 구조를 위한 real FFT를 사용합니다. 이후 두 갈래의 출력값을 합쳐서 결과를 출력합니다.</li> <li>FFC는 미분가능하며, convolutional layer에 1:1로 대응이 가능합니다. <ul> <li>초기 단계부터 이미지 전체를 이해하는 네트워크를 생성 가능하며, 이를 통해 효율적인 훈련이 가능합니다.</li> </ul> </li> <li>또한 receptive field가 이미지 전체를 덮을 수 있기 때문에, 고해상도 이미지에 효과적으로 적용가능합니다.</li> <li>결과를 확인해보면 벽돌, 사다리, 창문 등 <strong>인공적인 구조물 패턴에 잘 대응하는걸 볼 수 있습니다.</strong> </li> </ul> <p>FFC 중에서도 Real FFT의 상세 구조는 다음과 같습니다;</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ifft_shape_slice</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># RealFFT2d : Real(batch, c, h, w) --&gt; Complex(batch, c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="nf">rfftn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="sh">'</span><span class="s">ortho</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># ComplexToReal : Complex(batch, c, h, w/2) --&gt; Real(batch, 2c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">ffted</span><span class="p">.</span><span class="n">real</span><span class="p">,</span> <span class="n">ffted</span><span class="p">.</span><span class="n">imag</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">:])</span>

<span class="c1"># Convolutional Block : Real(batch, 2c, h, w/2) --&gt; Real(batch, 2c, h, w/2)
# 1*1 kernel이지만, 위 fft에서 변환된 이미지의 주파수 도메인에 대해 작업하므로, 
# 이미지 전역을 덮는 receptive field를 가집니다.
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv_layer</span><span class="p">(</span><span class="n">ffted</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">ffted</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">sel</span>
<span class="c1"># RealToComplex : Real(batch, 2c, h, w/2) --&gt; Complex(batch, c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:])</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">complex</span><span class="p">(</span><span class="n">ffted</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">ffted</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># InverseRealFFT2d : Complex(batch, c, h, w/2) --&gt; Real(batch, c, h, w)
</span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="nf">irfftn</span><span class="p">(</span><span class="n">ffted</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">ifft_shape_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="sh">'</span><span class="s">ortho</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th><img src="/assets/images/lama/lama-3.png" alt="모델구조"></th> </tr> </thead> <tbody> <tr> <td>모델 전체의 구조. 비교적 간단한 모래시계 모양의 구조를 가집니다.</td> </tr> </tbody> </table> <h3 id="모델-전반-구조">모델 전반 구조</h3> <ul> <li>입력 : 마스크로 덮힌 이미지 + 마스크 (크기 : 4<em>해상도</em>해상도)</li> <li>출력 : 마스크로 덮힌 부분이 칠해진 이미지 (크기 : 3<em>해상도</em>해상도)</li> <li>모델 : 모래시계 모양의 ResNet-structure 모델 <ol> <li>Reflection Padding + Conv-BatchNorm-ReLU Block (ksize=7)</li> <li>Downsampling Blocks * 3 <ul> <li>Conv-BatchNorm-ReLU Block (ksize=3, stride=2, padding=1)</li> <li>마지막 블록에서 feature를 local과 global로 나눔 <ul> <li>이때, <strong>local은 채널중 25%, global은 75%의 비율로 나눕니다.</strong> </li> </ul> </li> </ul> </li> <li>FFC Residual Blocks * 9 (big-LAMA 세팅이면 18) <ul> <li>마지막 블록에서 나눠진 local과 global을 다시 하나의 feature로 합침</li> </ul> </li> <li>Upsampling Blocks * 3 <ul> <li>ConvT-BatchNorm-ReLU Block (ksize=3, stride=2, padding=1)</li> </ul> </li> <li>Reflection Padding + Conv-BatchNorm-ReLU Block (ksize=7) + Sigmoid</li> </ol> </li> </ul> <hr> <h3 id="손실함수">손실함수</h3> <p>인페인팅 함수에서 손실함수를 정하는건, 가능한 출력물의 가짓수 때문에 매우 어려운 일이 됩니다.</p> <p>이를 위해 저자들은 Perceptual Loss 중에서도 특별한 형태의 손실함수를 사용합니다.</p> <ul> <li>기존의 supervised loss의 경우, 마스크가 이미지 대부분을 덮고 있을때 모델의 학습을 어렵게 하기 때문에 출력물의 해상도가 떨어져서 나오는 경우가 많았습니다.</li> <li>이에 비해 <strong>perceptual loss</strong>는 사전에 훈련된 네트워크 (ResNet50 기반)를 통해 이미지간의 feature distance를 측정하기 때문에, <strong>큰 마스크에도 상대적으로 잘 되며 다양한 출력물을 지원합니다.</strong> </li> <li>Perceptual Loss에서 쓰일 네트워크도 초반 레이어의 넓은 receptive field와, 이를 통한 거시적인 이미지 구조에 대한 이해가 필요하기 때문에, 저자들은 <strong>High Receptive Field Model (이하 HRFM)을</strong> 새로 설계하여 perceptual loss를 구합니다. <ul> <li>Fourier / Dilated convolution을 기존 convolution 대신 사용</li> </ul> </li> <li>해당 HRFM이 어떻게 훈련되었는지도 중요합니다. <ul> <li>Segmentation-based 일 경우, 배경의 사물같은 고레벨 구성요소 정보에 대해 집중할 수 있습니다.</li> <li>Classification-based일 경우, 배경 텍스쳐같은 거시적 정보에 집중할 수 있습니다.</li> <li>선택과 집중의 문제</li> </ul> </li> </ul> <p>또한 <a href="https://github.com/phillipi/pix2pix" rel="external nofollow noopener" target="_blank">Pix2Pix</a>에 영향을 받은 Discriminator를 추가해서 <strong>Adversarial Loss를</strong> 손실함수에 포함시킵니다.</p> <ul> <li>해당 Discriminator는 <strong>local patch 단계에서 계산해, 해당 patch가 마스크로 덮힌 부분인지 아닌지를 분별합니다.</strong> </li> <li>Adversarial Loss는 <strong>Non-saturating adversarial loss의</strong> 형태로 사용됩니다.</li> <li>또한, Discriminator에 대한 feature-matching loss를 추가로 계산해, GAN 훈련의 안정성을 늘립니다.</li> <li>이 두 손실함수를 통해 자연스럽게 원본 이미지와 맞물리는 이미지를 생성할 수 있습니다.</li> </ul> <p>마지막으로 R1 gradient penalty 함수를 추가하는걸로 손실함수를 완성시킵니다.</p> <hr> <table> <thead> <tr> <th><img src="/assets/images/lama/lama-8.png" alt="데이터셋 예시"></th> </tr> </thead> <tbody> <tr> <td>훈련용 데이터셋 마스크 생성 예시. LaMa는 밑의 두 행처럼 마스크를 생성합니다. 보시는 것처럼 기존보다 더 크고 다양한 면적의 마스크들을 볼 수 있습니다.</td> </tr> </tbody> </table> <h3 id="데이터셋-마스크-생성">데이터셋 마스크 생성</h3> <p>다양한 형태의 입력 마스크에 성공적으로 대응하기 위해서는 훈련 단계에서 다양한 마스크를 생성해서 같이 훈련시키는 것이 중요합니다.</p> <ul> <li>저자들은 이를 위해 기존보다 <strong>더 크고 다양한 면적을 덮는 마스크 생성법을</strong> 사용합니다.</li> <li>이를 위해 두 가지 방식으로 마스크를 생성하는데, 하나는 polygonal chain에서 크기를 더 키운 wide mask 방식이고, 다른 하나는 임의의 비율을 가진 직사각형을 사용한 box mask 방식입니다.</li> <li>이때, 마스크가 이미지의 50% 이상을 넘지 않도록 합니다.</li> </ul> <hr> <table> <thead> <tr> <th><img src="/assets/images/lama/lama-4.png" alt="실험결과"></th> </tr> </thead> <tbody> <tr> <td>서로 다른 데이터셋과 마스크 종류에 따른 인페인팅 모델들의 결과 비교. LaMa는 거의 모든 결과에 대해 타 모델 대비 우위를 차지하는걸 확인할 수 있습니다.</td> </tr> </tbody> </table> <h3 id="실험-및-결과">실험 및 결과</h3> <p>LaMa 모델은 실험에서 모델 크기가 몇배는 큰 모델들과의 비교에서도 LPIPS / FID metric면에서 훨씬 좋은 결과를 보여줍니다.</p> <ul> <li>비교 실험에서 metric면에서 (부분적으로) LaMa 보다 나은 모델들이었던 <a href="https://github.com/zsyzzsoft/co-mod-gan" rel="external nofollow noopener" target="_blank">CoModGAN</a>과 <a href="https://github.com/MADF-inpainting/Pytorch-MADF" rel="external nofollow noopener" target="_blank">MADF</a>는 LaMA 대비 3~4배는 더 큰 모습을 보이며, segmentation mask를 대상으로 한 실험에서는 LaMa 보다 쳐지는 모습을 보입니다.</li> </ul> <table> <thead> <tr> <th><img src="/assets/images/lama/lama-5.png" alt="FFC 유효성"></th> </tr> </thead> <tbody> <tr> <td>(훈련에 사용한 입력 해상도보다 큰 해상도의) 고화질 이미지를 대상으로한 인페인팅. FFC를 사용한 LaMa 외의 다른 모델들은 모두 인페인팅 결과가 부정확한 걸 확인할 수 있습니다.</td> </tr> </tbody> </table> <p>Ablation Study 결과;</p> <ul> <li>FFC의 유무는 특히 고화질 이미지를 대상으로한 inference에서 눈에 띄며, 창문이나 철조망 같은 반복적 구조의 inpainting에 큰 영향을 끼칩니다. <ul> <li>Dilated Convolution의 경우, FFC와 비슷한 성질을 가지며 (어느 정도는) 대용으로 사용이 가능하지만, <strong>receptive field가 더 제한이 되어</strong> 고화질 이미지를 대상으로 적용이 어렵다는 단점을 가집니다.</li> </ul> </li> <li>Perceptual Loss의 기본이 되는 모델의 선정은 최종 결과에 중요하게 적용됩니다. <ul> <li>논문에서는 <strong>ResNet50 기반 Segmentation + ADE20K</strong> 데이터셋 훈련 모델을 채택했습니다.</li> </ul> </li> </ul> <hr> <table> <thead> <tr> <th><img src="/assets/images/lama/lama-7.png" alt="부정적 lama 예시"></th> </tr> </thead> <tbody> <tr> <td>부정적인 결과 예시. 인물을 지운 부분에 흐릿한 artifact가 생기는걸 볼 수 있으며, 이는 데이터셋 분포에 포함되어 있지 않던 복잡한 배경 및 마스크 크기 때문입니다.</td> </tr> </tbody> </table> <h3 id="결론">결론</h3> <p>저자들은 FFC 레이어와 특수 perceptual loss, 마스크 데이터의 생성을 통해 단순한 모델구조로도 다양한 상황에 쉽게 사용될수 있는 image inpainting 모델을 제시했습니다.</p> <p>아직 나름대로의 한계 (데이터셋 분포에 있지 않은 이미지들에 대한 인페인팅, 사람 몸같은 큰 대상의 인페인팅시 왜곡 현상)등이 있지만, 현재까지 나온 모델들 중에서 가장 자연스럽게 인페인팅이 가능한 모델이라고 생각됩니다.</p> <p>저자들은 또한 FFC나 Dilated Convolution 뿐만이 아닌, Vision Transformer의 사용도 염두에 두면서 향후 이미지 인페인팅 연구에 대한 기대감을 심어줍니다.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/BirthAndDeathOfRose/">[Paper Review] Birth and Death of a Rose</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Weekly-Notes/">[Paper Review] LT3SD : Latent Trees for 3D Scene Diffusion</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Animate3D/">[Paper Review] Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/TCP/">Towards Controllable and Photorealistic Region-wise Image Manipulation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Swapping_Autoencoder/">Swapping Autoencoder for Deep Image Manipulation</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Patrick Kwon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>