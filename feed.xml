<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yj7082126.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yj7082126.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-13T01:42:34+00:00</updated><id>https://yj7082126.github.io/feed.xml</id><title type="html">Patrick Kwon (권용재)</title><subtitle>Academic Portfolio webpage for Patrick Kwon (권용재), Deep learning Researcher at Naver Webtoon
</subtitle><entry><title type="html">Birth and Death of a Rose</title><link href="https://yj7082126.github.io/blog/2025/BirthAndDeathOfRose/" rel="alternate" type="text/html" title="Birth and Death of a Rose" /><published>2025-01-10T13:10:00+00:00</published><updated>2025-01-10T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2025/BirthAndDeathOfRose</id><content type="html" xml:base="https://yj7082126.github.io/blog/2025/BirthAndDeathOfRose/"><![CDATA[<h2 id="birth-and-death-of-a-rose">Birth and Death of a Rose</h2>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Birth and Death of a Rose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Authors</td>
      <td>Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</td>
    </tr>
    <tr>
      <td>Institute</td>
      <td>Stanford University</td>
    </tr>
    <tr>
      <td>Conference</td>
      <td>arxiv (2412.05278)</td>
    </tr>
    <tr>
      <td>Links</td>
      <td><a href="https://arxiv.org/abs/2412.05278">[ArXiV]</a> <a href="https://chen-geng.com/rose4d">[Project]</a></td>
    </tr>
    <tr>
      <td>Tl;dr</td>
      <td>4D Reconstruction 계열 논문. 장미가 피고 지는것처럼 사물의 모양이나 색깔등이 시간에 따라 변하는걸 Temporal Object Intrinsic 으로 정의, 이를 DINOv2 feature map 의 형태를 띈 Neural State Map 의 형태로 구현한다. 카메라 각도와 시간 입력을 받아 Neural State Map 을 출력하는 Neural Template 을 배우고, 이 모델을 바탕으로 SDS를 실행, 시간에 따라 변하는 3D 모델을 만든다</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/img/20250110/image.png" alt="image.png" /></p>

<p><img src="/assets/img/20250110/image%201.png" alt="image.png" /></p>

<p><img src="/assets/img/20250110/image%202.png" alt="image.png" /></p>

<h2 id="정리">정리</h2>

<p>| 무엇을 하고자 하는가 | 장미가 피고 지거나 양초가 녹는 것처럼, 시간에 따른 사물의 변화를 적용할 수 있는 3D (4D) 생성을 생성모델의 힘을 빌려 진행
새로운 정의 :</p>
<ul>
  <li>Temporal Object Intrinsic (TOI) : 시간에 따른 사물 본질의 변화 |
| — | — |
| 왜 이전에는 해결되지 않았나 | SDS는 사물의 relighting을 지원하지 않으며, 모션도 적용시킬수 없고, 무엇보다 Janus Problem 으로 보여지듯 3D 를 잘 모른다. 당연하지만 4D는 더 못할거다.
따라서 SDS를 사용하되, 카메라 각도와 시간에 대해 변하는 TOI 정보에 대해 gradient가 먹히도록 하는 새로운 구조가 필요하다.
새로운 정의 :</li>
  <li>Neural State Map (NSM) : 위의 TOI 를 내포하여 TOI 를 3D 상으로 구현시킬수 있는 representation. 이 연구에선 DINO v2 Feature Map 을 바탕으로 함.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Neural Template (NT) : 입력 카메라 각도와 시간에 대해 3D 모델에 대한 NSM 을 뱉어주는 모델</td>
          <td> </td>
          <td> </td>
        </tr>
        <tr>
          <td> </td>
          <td>어떻게 해결하였나</td>
          <td>1. 한 사물에 대한 Coarse Deformable Mesh (CDM) 를 생성한다.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>디테일할 필요는 없고, 사물의 모양과 시간에 따라 바뀌는 특성이 반영된 수준이면 충분하다.</li>
  <li>CogVideoX를 통해 초기 비디오를 생성.</li>
  <li>이 비디오의 canonical 프레임에 대해 Zero123, Imagedream 등을 사용해 초기 모델을 학습한다.</li>
  <li>다른 시간의 프레임들에 대해 위의 모델을 변형시킬수 있는 Deformation Field 를 학습 (Optical Flow loss, ARAP 등을 사용)
    <ol>
      <li>위의 CDM 에 대한 NT를 배운다</li>
    </ol>
  </li>
  <li>각 카메라 각도와 시간에 대해 CDM 을 렌더링한다.</li>
  <li>3D Recon 을 통해 배운 모델들은 실제 이미지 분포 (DINOv2) 와 다를 가능성이 크다. 이를 해결하기 위해 LCM 모델을 사용해 렌더링된 이미지를 한번 고쳐준다.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>이 이미지들에 대해 DINOv2 를 실행, 위의 NSM을 얻는다</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="느낀점">느낀점</h2>

<ul>
  <li>지금까지 비디오 생성 모델 기반으로 본것중에선 새로운 관점? (물론 이 연구자가 이쪽으로 많이 보긴 했다)</li>
</ul>

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{geng2024birthdeathrose,
      title={Birth and Death of a Rose}, 
      author={Chen Geng and Yunzhi Zhang and Shangzhe Wu and Jiajun Wu},
      year={2024},
      eprint={2412.05278},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.05278}, 
}
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><category term="3D Generation" /><category term="4D Generation" /><summary type="html"><![CDATA[Birth and Death of a Rose]]></summary></entry><entry><title type="html">SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</title><link href="https://yj7082126.github.io/blog/2024/SynCamMaster/" rel="alternate" type="text/html" title="SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints" /><published>2024-12-15T13:10:00+00:00</published><updated>2024-12-15T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2024/SynCamMaster</id><content type="html" xml:base="https://yj7082126.github.io/blog/2024/SynCamMaster/"><![CDATA[<h2 id="syncammaster">SynCamMaster</h2>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Authors</td>
      <td>Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</td>
    </tr>
    <tr>
      <td>Institute</td>
      <td>Zhejiang University, Kuaishou Technology, Tsinghua University, CUHK</td>
    </tr>
    <tr>
      <td>Conference</td>
      <td>arxiv (2412.07760)</td>
    </tr>
    <tr>
      <td>Links</td>
      <td><a href="https://arxiv.org/abs/2412.07760">[ArXiV]</a> <a href="https://jianhongbai.github.io/SynCamMaster/">[Project]</a> <a href="https://github.com/KwaiVGI/SynCamMaster">[Code]</a></td>
    </tr>
    <tr>
      <td>Tl;dr</td>
      <td>복수의 카메라 구도에 대해 똑같은 내용이 담긴 영상을 생성하는 기술</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/img/20241215/image.png" alt="image.png" /></p>

<h2 id="정리">정리</h2>

<p>| 무엇을 하고자 하는가 | 다중 카메라 구도에서부터의 영상 생성  |
| — | — |
| 왜 이전에는 해결되지 않았나 | 1. 복수의 카메라 구도에서부터의 똑같은 내용에 대한 영상 생성을, 4D 일관성을 지키면서 생성하는것</p>
<ol>
  <li>1을 이루기 위한, 다양한 다중 카메라 영상 데이터셋의 부재 |
| 어떻게 해결하였나 | 거대 T2V 모델에 기반하며, 다양한 어댑터 모델들을 적용
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Camera Encoder : 각 카메라들의 [R</td>
              <td>t] 입력을 인코딩</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>Multi-view Synchronization Model : DiT 의 각 Transformer 레이어에 부착, 4D Consistency 를 유지한다</li>
    </ul>
  </li>
</ol>

<p>신종 데이터셋 제작 : 언리얼 엔진 기반의 다중 카메라 영상 데이터셋 + 공개되어있는 다중 이미지, 단일 영상등으로 구성</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Hybrid Data-training 으로 훈련 최적화</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="기존-유사-연구">기존 유사 연구</h3>

<p>| Name | Cite | Context |
| — | — | — |
| <a href="https://github.com/Stability-AI/generative-models">SV4D</a> | Xie et al. 2024 | * 4D 사물 제작에 특화, 고정된 카메라 각도에서만 동작 가능, 실제 영상의 환경과 괴리감이 큼</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>이미지 생성 모델의 3D prior + 비디오 생성 모델의 Motion prior</td>
          <td> </td>
          <td> </td>
          <td> </td>
        </tr>
        <tr>
          <td> </td>
          <td><a href="https://github.com/hi-zhengcheng/vividzoo">Vivid-ZOO</a></td>
          <td>Li et al. 2024</td>
          <td>* 4D 사물 제작에 특화, 고정된 카메라 각도에서만 동작 가능, 실제 영상의 환경과 괴리감이 큼</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>LoRA 훈련을 통해 (3D 사물 / 영상간) domain misalignment 을 해결</td>
          <td> </td>
          <td> </td>
          <td> </td>
        </tr>
        <tr>
          <td> </td>
          <td><a href="https://github.com/CollaborativeVideoDiffusion/CVD">CVD</a></td>
          <td>Kuang et al. 2024</td>
          <td>* CameraCtrl 확장, 같은 출발점에서 부터의 다양한 카메라 경로에 대해 생성</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>데이터셋 문제로 인해 좁은 범위의 카메라 경로만 가능</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>SynCamMaster는 시간축에 대한 카메라 경로 편집이 아닌, 복수의 카메라 구도에 대한 생성을 목표로 함</td>
          <td> </td>
          <td> </td>
          <td> </td>
        </tr>
        <tr>
          <td> </td>
          <td><a href="https://github.com/basilevh/gcd">GCD</a></td>
          <td>Van Hoorick et al. 2024</td>
          <td>* 입력 영상과 새로운 카메라 각도에 대해서 새로운 출력 영상을 생성</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>SynCamMaster 는 영상 입력이 아닌 텍스트 입력을 추구함. 해당 목표 달성 이후 Novel View 도 겸사겸사…</td>
          <td> </td>
          <td> </td>
          <td> </td>
        </tr>
        <tr>
          <td> </td>
          <td><a href="https://github.com/facebookresearch/ViewDiff">ViewDiff</a></td>
          <td>Hollein et al. 2024</td>
          <td>* 입력 이미지와 새로운 카메라 각도에 대해서 새로운 출력 이미지를 생성 (유사 시도들과는 다르게, 배경이 가능)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>영상 단위나 전경 레벨에선 문제가 여전함</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="방법론--multi-view-synchronization-module">방법론 : Multi-View Synchronization Module</h3>

<p>DiT 기반 T2V 모델 (주로 CogVideoX) 의 생성 능력을 그대로 살리면서, 복수의 카메라 구도에 대해서 동시에 생성할 수 있게 해주는 어뎁터 모듈을 각 DiT 블록마다 붙인다.</p>

<p>입력 : (n : 구도 수, f : 프레임 수, s : 프레임 크기 (w * h))</p>

<ul>
  <li>원 입력 Spatial Feature F_s (n * f * s * d)</li>
  <li>Camera Params cam (n * 12)
    <ul>
      <li>n 개의 카메라 각도중 하나를 global 로 잡고 나머지 normalize 함</li>
      <li>(p.18 에선 extrinsic 과 plucker embedding 를 사용하는것의 차이에 대해 다루었는데, 큰 차이가 없었다고 결론지었음)</li>
    </ul>
  </li>
</ul>

<p>출력 :</p>

<ul>
  <li>다중 카메라 구도에 최적화된 Spatial Feature F^\bar{v}</li>
</ul>

<p>방법론: (구도 i 에 대해서 실행)</p>

<ol>
  <li>카메라 인코더
    <ol>
      <li>FC 레이어 (12 채널 —&gt; d 채널) (zero. init)</li>
      <li>Camera Param 을 d 채널로 바꾸고, (* s) 만큼 repeat</li>
      <li>이후 F_s 에 element-wise addition</li>
    </ol>
  </li>
  <li>Cross-view Self Attention Layer
    <ol>
      <li>모든 구도에서의 Feature 를 다 같이 참고한다</li>
      <li>원 모델의 Temporal Attention 레이어 weight 로 init</li>
      <li>이후 projection (Linear+ReLU) (zero init) 를 해서 더한다</li>
    </ol>
  </li>
</ol>

<h3 id="방법론--training-dataset--training-strategy">방법론 : Training Dataset &amp; Training Strategy</h3>

<p>총 3종류의 데이터로 구성됨</p>

<ol>
  <li>언리얼 엔진으로 자체 구축된 Multi View Video 데이터
    <ol>
      <li>500개의 3D 배경, 70개의 3D 에셋 (사람, 동물 등) 중에 1~2개 선택 후 애니메이션</li>
      <li>각 배경당 36개의 카메라, 100개의 프레임 렌더링
        <ol>
          <li>(중앙 기준 3.5~9m, elevation 0~45 에서 랜덤 샘플링)</li>
        </ol>
      </li>
      <li>훈련에서 60%의 확률로 사용</li>
    </ol>
  </li>
  <li>Single View Video 에서 뽁은 Multi View Image 데이터
    <ol>
      <li>RealEstate-10K, DL3DV-10K 는 카메라 경로 정보가 있는 비디오로 구성됨</li>
      <li>영상에서 프레임들 샘플링해서 사용
        <ol>
          <li>(프레임간 차이가 100을 넘지 않도록 해서 겹치는 구역이 있도록 함)</li>
        </ol>
      </li>
      <li>훈련에서 20%의 확률로 사용</li>
      <li>Generalization 에 크게 기여</li>
    </ol>
  </li>
  <li>Single View Video (High Quality, 온라인에서 모음) (카메라 정보 없음)
    <ol>
      <li>Regularization 으로 사용</li>
      <li>SynCam 은 고정된 카메라 구도를 상정했기 때문에 카메라 움직임이 크거나 static 한 영상은 배제해야 함
        <ol>
          <li>downsampling 된 영상의 첫 프레임에 SAM 을 통해 객체별 seg.mask 확보</li>
          <li>CoTracker 를 통해 비디오 내에서 seg.mask 들의 움직임을 계산</li>
          <li>특정 threshold 미만의 영상들 모두 배제</li>
          <li>최종적으로 12,000개 영상 선별</li>
        </ol>
      </li>
      <li>훈련에서 20% 확률로 사용</li>
    </ol>
  </li>
</ol>

<p>훈련시, 처음에는 카메라 구도간 차이가 적은 세팅으로 시작해서 점진적으로 크게 가도록</p>

<ul>
  <li>첫 10k : 0 ~ 60도, 10~20k : 30 ~ 90도, 20k~ : 60 ~ 120도</li>
</ul>

<p>총 50K 훈련, 해상도 (384 * 672), learning rate 1e-4, batch size 32</p>

<h3 id="방법론--novel-view-video-synthesis">방법론 : Novel View Video Synthesis</h3>

<p>원 세팅 : 입력 텍스트 + 복수의 카메라 구도</p>

<p>새로운 세팅 : 입력 텍스트 + 입력 비디오 + 복수의 카메라 구도</p>

<p>훈련시, 90% 확률로 첫번째 카메라 구도의 latent 를 입력 비디오의 latent로 교체</p>

<p>이후 IP2P 처럼 비디오에 대한 cfg 를 계산해서 적용 (텍스트 : 7.5, 비디오 : 1.8)</p>

<h2 id="느낀점">느낀점</h2>

<ul>
  <li>Data Collection 과정이 중요, 어떤 데이터를 어떻게 사용하는지로 논문의 평가가 달라질거라 생각됨</li>
  <li>거대 비디오 모델 위에 올라타는 형태의 논문은 모델 구성에 한계가 있다고 생각되는데, 그럼 어떤 과정을 거쳐야 독창적인 연구를 제시할 수 있을까?</li>
</ul>

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{bai2024syncammaster,
      title={SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints}, 
      author={Jianhong Bai and Menghan Xia and Xintao Wang and Ziyang Yuan and Xiao Fu and Zuozhu Liu and Haoji Hu and Pengfei Wan and Di Zhang},
      year={2024},
      eprint={2412.07760},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.07760}, 
}
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><category term="Video Generation" /><summary type="html"><![CDATA[SynCamMaster]]></summary></entry><entry><title type="html">Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</title><link href="https://yj7082126.github.io/blog/2024/Animate3D/" rel="alternate" type="text/html" title="Animate3D: Animating Any 3D Model with Multi-view Video Diffusion" /><published>2024-09-20T13:10:00+00:00</published><updated>2024-09-20T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2024/Animate3D</id><content type="html" xml:base="https://yj7082126.github.io/blog/2024/Animate3D/"><![CDATA[<h2 id="animate3d-animating-any-3d-model-with-multi-view-video-diffusion">Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</h2>

<p>Authors : Quan Meng, Lei Li, Matthias Nießner, Angela Dai 
(CASIA, DAMO Academy, Alibaba Group, Hupan Lab)</p>

<p><a href="https://animate3d.github.io/">[Project Page]</a> 
<a href="https://arxiv.org/pdf/2407.11398">[Paper]</a></p>

<p>Despite various research in <strong>dynamic 3D content generation</strong> (4D Generation) there hasn’t been a singular foundation model. Separately learning spatial factors from 3D models and temporal motions from video models result in quality degradation (e.g. SVD + Zero-123), and animating 3D objects usually fails to preserve multi-view attributes.</p>

<p>Animate3D suggests to animate any 3D models with unified spatiotemporal consistent supervision. The process first starts with MV-VDM, a foundational 4D model based from MVDream and a spatiotemporal motion module, focused on learning natural dynamic motions. A MV2V-Adapter, adapted from I2V-Adapter, is also used to handle multi-view image conditions. For 3D context, 4DGS is jointly optimized through both reconstruction and 4D Score Distillation Sampling. For training, the authors also create MV-Video, a large-scale multi-view video dataset that consists about 1.8M multi-view videos.</p>

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{
	jiang2024animate3d,
	title={Animate3D: Animating Any 3D Model with Multi-view Video Diffusion},
	author={Yanqin Jiang and Chaohui Yu and Chenjie Cao and Fan Wang and Weiming Hu and Jin Gao},
	booktitle={arXiv},
	year={2024},
}
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><category term="3D Generation" /><category term="4D Generation" /><summary type="html"><![CDATA[Animate3D: Animating Any 3D Model with Multi-view Video Diffusion]]></summary></entry><entry><title type="html">LT3SD : Latent Trees for 3D Scene Diffusion</title><link href="https://yj7082126.github.io/blog/2024/Weekly-Notes/" rel="alternate" type="text/html" title="LT3SD : Latent Trees for 3D Scene Diffusion" /><published>2024-09-20T13:10:00+00:00</published><updated>2024-09-20T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2024/Weekly%20Notes</id><content type="html" xml:base="https://yj7082126.github.io/blog/2024/Weekly-Notes/"><![CDATA[<h2 id="lt3sd--latent-trees-for-3d-scene-diffusion">LT3SD : Latent Trees for 3D Scene Diffusion</h2>

<p>Authors : Quan Meng, Lei Li, Matthias Nießner, Angela Dai 
(Technical University of Munich)</p>

<p><a href="https://quan-meng.github.io/projects/lt3sd">[Project Page]</a> 
<a href="https://arxiv.org/pdf/2409.08215">[Paper]</a> 
<a href="https://github.com/quan-meng/lt3sd">[Code]</a></p>

<p float="left">
  <img src="/assets/img/20240920/LT3SD-2-Figure2-1.png" width="50%" />
  <img src="/assets/img/20240920/LT3SD-3-Figure3-1.png" width="50%" /> 
</p>

<p>Most of previous models focus on object-level generation, which assumes a fixed orientation and bounded space for class of objects, and is totally not the case for 3D scenes (high resolution, unstructured geometries, diverse object arrangements, varying spatial extents)</p>

<p>LT3SD introduces a <strong>novel latent-tree representation</strong> , a hierarchical decomposition with a series of <strong>geometry (lower freq)</strong> and <strong>latent feature (higher freq)</strong> encodings that boasts a more compact, effective representation compared to single latent codes or latent pyramid.</p>

<p>Specifically, the target ‘scene’ is represented as a truncated <strong>unsigned distance field</strong> (TUDF), which is then passed into a encoder implemented as <strong>3D CNNs</strong> to give a low-dimensional TUDF as geometry volume, and a latent feature volume for higher frequency details. This procedure is repeated for N(=3 in the paper) steps to give a <strong>latent tree of 3 levels</strong>. For diffusion, a latent diffusion approach is used with a 3D UNet as its base. The inference structure allows a <strong>patch-by-patch scene generation</strong> (based on diffusion inpainting) and a coarse-to-fine refinement (adapting MultiDiffusion for speedup) to generate almost infinite-sized scenes.</p>

<p>The model was trained based on the 3D-FRONT dataset (6,479 houses), processed as UDFs with voxel size of 0.022m and random crops. The encoder/decoder training takes 5 hr ~ 1 day for single RTX A6000 GPU, and the diffusion training takes approx. 6 days for two A6000 GPUSs. Compared against PVD, NFD, and BlockFusion under various metrics, LT3SD (N=3) displayed better quantitative scores.</p>

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{meng2024lt3sdlatenttrees3d,
	title={LT3SD: Latent Trees for 3D Scene Diffusion}, 
	author={Quan Meng and Lei Li and Matthias Nießner and Angela Dai},
	journal={arXiv preprint arXiv:2409.08215},
	year={2024}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><category term="Scene Generation" /><summary type="html"><![CDATA[LT3SD : Latent Trees for 3D Scene Diffusion]]></summary></entry><entry><title type="html">Towards Controllable and Photorealistic Region-wise Image Manipulation</title><link href="https://yj7082126.github.io/blog/2022/TCP/" rel="alternate" type="text/html" title="Towards Controllable and Photorealistic Region-wise Image Manipulation" /><published>2022-03-17T13:10:00+00:00</published><updated>2022-03-17T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2022/TCP</id><content type="html" xml:base="https://yj7082126.github.io/blog/2022/TCP/"><![CDATA[<h2 id="towards-controllable-and-photorealistic-region-wise-image-manipulation">Towards Controllable and Photorealistic Region-wise Image Manipulation</h2>

<table>
  <thead>
    <tr>
      <th>이름</th>
      <th><strong>Towards Controllable and Photorealistic Region-wise Image Manipulation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>저자</td>
      <td><strong>Ansheng You</strong>, <strong>Chenglin Zhou</strong>, Qixuan Zhang, Lan Xu</td>
    </tr>
    <tr>
      <td>토픽</td>
      <td>Image Editing</td>
    </tr>
    <tr>
      <td>년도</td>
      <td>2021.08</td>
    </tr>
    <tr>
      <td>학회</td>
      <td>2021 ACM Multimedia</td>
    </tr>
    <tr>
      <td>링크</td>
      <td><a href="https://arxiv.org/abs/2108.08674">[논문]</a></td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/tcp/teaser.png" alt="Image Texture Swapping 예시" /></p>

<p>(본 논문은 Swapping Autoencoder 논문의 직접적인 개선을 목표로 했기 때문에, 해당 정리를 읽으시기 전에 <a href="_posts/2022-03-08-Swapping_Autoencoder.md">Swapping Autoencoder 리뷰</a> 를 먼저 읽으시는 걸 추천드립니다.)</p>

<h2 id="intro">Intro</h2>

<p>Swapping Autoencoder (이하 Swap.Ae) 는 딥러닝 기반 이미지 편집을, 이미지를 <em>미시적으로 표현하는 2차원적 구조적 정보</em> (이하 <strong>content</strong>, Swap.Ae 에서는 structure로 표현)와, <em>거시적으로 표현하는 1차원적 스타일, 텍스쳐 정보</em> (이하 <strong>style</strong>, Swap.Ae 에서는 texture로 표현)으로 <strong>나눠 생각해서 해결</strong>하고자 했습니다.<br />
(가령 성당 이미지의 경우, 성당의 위치와 구조물의 생김새, 창문 등의 디자인적 요소는 content에 해당하며, 이미지의 색조나 채도, 건물의 질감 등의 요소는 style에 해당합니다.)<br />
(다만 이미지에 따라 해당 두 요소가 항상 말끔하게 떨어지는건 아니며, 그럼에도 최대한 말이 되는 방향으로 나누는 것이 이 모델의 목표입니다.)</p>

<p>이 두 요소가 나눠진 상태에서 <em>content / style 에 해당하는 Latent code를 서로 다른 이미지들끼리 바꾸는 것으로 image translation</em> 이 가능하고<br />
(예 : 봄을 배경으로 한 들판에서 content를, 눈 덮인 설산을 style 로 삼는 것으로 눈이 덮인 들판 이미지를 생성 가능), 나아가서 2차원 content space 상에서 latent code를 조작하는 것으로 image editing이 가능합니다.<br />
(예 : 산과 강이 있는 이미지에서 강에 해당하는 content 부분을 산이 있는 곳에 옮겨다 붙여넣는 걸로 강의 크기를 키우는 것이 가능).</p>

<p>이러한 editing이 자연스럽게 성립하기 위해서는 content와 style간의 분리가 제대로 된 것이 중요합니다; 그렇지 못한 경우, image translation 결과가 content 나 style 둘 중 하나에만 크게 영향을 받게 되어 좋은 결과가 나오기 어려울 것입니다. 저자들은 Swapping Autoencoder의 훈련이 대체로 잘되는 편이지만, 여러 인퍼런스 결과에서 content / style 간 분리가 제대로 되지 않아 구조적 일관성이 Image translation 문제에서 잘 지켜지지 않는 등, <strong>분리를 위한 추가적인 조치들이 필요함을 지적합니다.</strong></p>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/tcp/false.png" alt="fail case" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Swapping Autoencoder 실패 케이스의 한 예시. Content / Style 중에서 Style에 비중이 더 쏠려 있어 의미있는 편집이 어려운게 확인됩니다.</td>
    </tr>
  </tbody>
</table>

<p>이를 위해 저자들은 <strong>self-supervised training에 기반한 2개의 loss</strong>를 추가적으로 제시합니다. 첫 번째로 <strong>code consistency loss</strong>는, 서로 다른 이미지들의 content / style code 로 생성된 이미지를 다시 encoder에 넣어서 content / style code를 획득했을 때 뽑힌 latent code 들이 입력에 사용한 code 들과 같은지를 계산합니다. 이 loss를 통해 content / style 의 일관성을 추가적으로 확보할 수 있었습니다.</p>

<p>두 번째로 <strong>content alignment loss</strong>는, 이미지의 마스크 단위 변환을 위해서 추가된 loss 입니다. (예 : 얼굴의 피부나 눈, 입술만 선택적으로 바꾸는 경우) 훈련과정에서 랜덤하게 생성한 흑백 마스크를 통해 두 이미지의 latent code들을 합치고, 이렇게 생성된 이미지가 (마스크 범위 안에서) 얼마나 이전 이미지들과 같은지를 계산해서, 구하기 힘든 semantic image dataset 없이도 비슷한 훈련을 수행할 수 있게끔 합니다.</p>

<p>이 두개의 loss term을 추가한 모델을 저자들은 <strong>TCP</strong>라고 부르며, FFHQ, LSUN-Church 같은 데이터셋에 대해 높은 성과를 기록한다고 주장합니다.</p>

<h2 id="모델-구조">모델 구조</h2>

<p>기본적인 모델 구조는 Swapping Autoencoder에서 크게 달라지지 않았기 때문에, 
모델 구조에 대한 설명은 <a href="_posts/2022-03-08-Swapping_Autoencoder.md">Swapping Autoencoder 리뷰</a> 을 참고하시기 바라며,<br />
해당 리뷰에서는 새롭게 제안된 2개의 loss에 대해서만 적도록 하겠습니다.</p>
<ul>
  <li>논문에서 StyleGAN2 Generator 구조를 따라갔다고 서술하는데, swap.ae 에서는 StyleGAN2에 여러 변형을 거친 (예 : output skip 제거, residual block 구조), StyleGAN2 Discriminator 구조와 유사한 모양을 가지고 있어 다를 수 있습니다.</li>
</ul>

<h3 id="code-consistency-loss">Code consistency loss</h3>

<p>기존의 Swap.Ae 는 Encoder 단계에서 style에서의 구조적 정보를 없애기 위해. average pooling을 통해 style을 1차원 벡터로 만들어 (비교적 단순하게) content / style간 분리를 진행했습니다. 그러나 앞선 예시에서 보인것처럼 여전히 style에 따라 구조적인 정보가 변하는 경우들이 존재하고, 저자들은 이는 content의 정보 상당수가 style로 섞여 들어가기 때문에 발생한다고 주장합니다.</p>

<p><a href="https://github.com/AliaksandrSiarohin/wc-gan">Whitening and Coloring Batch Transform for GANs</a> 에서 제시된 feature whitening을 content code에 사용해 content / style 간 분리를 강화할 경우, 중요한 구조적 정보가 사라진다는 문제가 발생하기 때문에 사용하지 않았습니다.</p>

<p>대신, 저자들은 비교적 단순한 self-supervised 방법론을 사용합니다; Encoder가 convergence를 이루었으면, 이미지 A에서 나온 content code와 B에서 나온 style code를 합쳤을 때 생성된 이미지 C를 다시 Encoder에 넣었을때, 추출된 content code 는 생성에 사용한 A의 content code와 같아야 하고, style code 역시 생성에 사용한 B의 style code와 같아야 하는 겁니다.</p>

<p>위의 가설을 충족하기 위해선 Encoder가 convergence를 <em>이룬 후에</em> 해당 loss를 적용해야 하지만, 실제로는 K iterations (K=16) 마다 적용하는 것으로 해당 문제를 우회합니다. (진짜로?)</p>

\[\mathcal{L}_{CC} = \mathcal{L}_1(C_g, C_A) + \lambda\mathcal{L}_1(S_g, S_B)\]

<p><a href="https://github.com/junyanz/BicycleGAN">BicycleGAN</a> 에서 제시한 Conditional Latent Regression GAN 모델과 유사한 loss이지만, 정해진 Gaussian Distribution 이 아닌, 실제 입력값에서 나온 latent code를 기반으로 합니다.</p>

<h3 id="content-alignment-loss">Content alignment loss</h3>

<p>Region-wise style transfer는, binary mask 가 입력 이미지들과 같이 주어진 상태에서, 마스크로 가려진 부분은 원래의 content / style을 유지하고, 그렇지 않은 부분은 content를 유지하면서 style만 바꿔지는걸 목표로 합니다.</p>

<p>이론적으론 별다른 훈련 없이 global style transfer를 마친 이미지를 마스크로 곱해서 더하면 되지만, 그러할 경우 마스크의 경계선 부분에 artifact가 생기는걸 막을 수 없기 때문에 저자들은 훈련중에 입/출력 이미지가 아닌, feature map 차원에서부터 마스크를 적용하여 훈련하는 것으로 이 문제를 해결하고자 합니다. (이 때, 마스크는 랜덤하게 생성합니다.)</p>]]></content><author><name></name></author><category term="blog" /><category term="image-editing" /><summary type="html"><![CDATA[Towards Controllable and Photorealistic Region-wise Image Manipulation]]></summary></entry><entry><title type="html">Swapping Autoencoder for Deep Image Manipulation</title><link href="https://yj7082126.github.io/blog/2022/Swapping_Autoencoder/" rel="alternate" type="text/html" title="Swapping Autoencoder for Deep Image Manipulation" /><published>2022-03-07T13:10:00+00:00</published><updated>2022-03-07T13:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2022/Swapping_Autoencoder</id><content type="html" xml:base="https://yj7082126.github.io/blog/2022/Swapping_Autoencoder/"><![CDATA[<h2 id="논문스터디--swapping-autoencoder-for-deep-image-manipulation">논문스터디 : Swapping Autoencoder for Deep Image Manipulation</h2>

<table>
  <thead>
    <tr>
      <th>이름</th>
      <th><strong>Swapping Autoencoder for Deep Image Manipulation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>저자</td>
      <td><strong>Taesung Park</strong>, <strong>Jun-Yan Zhu</strong>, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, <strong>Richard Zhang</strong></td>
    </tr>
    <tr>
      <td>토픽</td>
      <td>Image Editing</td>
    </tr>
    <tr>
      <td>년도</td>
      <td>2020.07</td>
    </tr>
    <tr>
      <td>학회</td>
      <td>NeurIPS 2020</td>
    </tr>
    <tr>
      <td>링크</td>
      <td><a href="https://arxiv.org/abs/2007.00653v2">[논문]</a>, <a href="https://github.com/taesungp/swapping-autoencoder-pytorch">[코드1]</a>, <a href="https://github.com/rosinality/swapping-autoencoder-pytorch">[코드2]</a></td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/swap_ae/swap1.jpeg" alt="Image Texture Swapping 예시" /></p>

<h2 id="intro">Intro</h2>

<p><strong>Swapping Autoencoder</strong>는 여타 GAN Inversion / Editing 처럼, 실제 이미지에 대한 수정 을 가능케 합니다.<br />
이전까지 해당 분야의 많은 논문들은 이미지 수정 시 (의도되었든 의도되지 않았든) <em>입력 이미지에서만 존재하는 정보</em>와 <em>이미지 도메인 전반에 존재하는 정보</em>를 분리해서 생각해왔고,<br />
(예시 : Image Cartoonization 실행시, <em>입력 이미지의 얼굴 구조</em> 와 <em>카툰 도메인에서의 텍스쳐 및 구조 정보</em>),<br />
이를 위해 대체로 supervised training을 사용해 왔습니다.<br />
저자들은 이 문제를 <strong>unsupervised training</strong>으로 이 문제를 해결합니다.</p>

<p>해당 논문의 핵심은 AutoEncoder 구조상에서 <strong>이미지를 구조와 텍스쳐</strong>라는, 독립적인 두 요소로 나누어서 Image Encoding을 수행한 후,<br />
서로 다른 이미지에서 나온 구조와 텍스쳐를 합쳐서 Generation을 거쳐도 실제 이미지와 유사하도록 훈련하는 것에 있습니다.<br />
(논문에서 구조는 이미지 안에서의 시각적 패턴을, 텍스쳐는 그 외의 정보들을 뜻합니다. 해당 논문을 인용한 다른 논문들에서는 구조 (structure)를 content로, 텍스쳐 (texture)를 style로 쓰기도 하며, 구조와 텍스쳐가 일반적인 관념만큼 잘 떨어지는게 아니기 때문에 해석의 여지가 있습니다.<br />
(이 논문리뷰에서는 ‘구조’를 <em>미시적인 요소들로 구성된 2차원 정보</em>로, ‘텍스쳐’는 <em>거시적인 요소들로 구성된 1차원 정보</em>로 정의합니다.)</p>

<p>구조와 텍스쳐를 보다 잘 나누기 위해, 저자들은 <strong>이미지 패치 단위로 확인하는 Discriminator</strong>를 사용해서 구조 latent code는 구조를,<br />
텍스쳐 latent code는 텍스쳐를 encoding 하도록 강제합니다.</p>

<p>이런 구조를 통해, Swapping Autoencoder <strong>이미지 보간, 이미지 구조 / 텍스쳐 분리 및 수정, 이미지 편집</strong> 등 다양한 이미지 수정 임무를<br />
기존 유사한 시도들과 비교했을 때 시각적으로도 좋은 결과물을 효율적으로 생산할 수 있음을 보여줍니다.<br />
또한 저자들은 StyleGAN 같은 unconditional image generation 논문들(과 거기서 파생된 Inversion Methods)과 비교하면서,<br />
해당 논문들은 사전에 정의된 분포에서만 이미지를 생성하기 때문에 실제 이미지들에 잘 적용되지 않거나, 느리다는 점을 지적합니다.</p>

<h3 id="키워드">키워드</h3>
<ul>
  <li>미시적 구조와 거시적 텍스쳐의 분리된 encoding</li>
  <li>AutoEncoder 구조</li>
</ul>

<hr />
<h2 id="모델-구조">모델 구조</h2>

<p><img src="/assets/images/swap_ae/overview.jpeg" alt="Model overview" /></p>

<p>이름에서 보이듯, Swapping Autoencoder의 기본적인 구조는 <strong>Encoder와 Generator, Discriminator로 이루어진 Autoencoder</strong>입니다.<br />
이 때, Discriminator 디자인은 StyleGAN2에서, Encoder/Generator 블록 구조는 ResNet에서 사용했습니다.</p>

<p>저자들은 3가지 목적을 가지고 해당 모델의 구성 및 훈련 계획을 세웠습니다; 1. 정확한 이미지 재구성, 2. 서로 다른 요소들을 encoding하는 latent code, 3. 이미지 패치 단위 Discriminator를 통한 구조-텍스쳐 분리.<br />
이중 첫 번째 목표인 이미지 재구성의 경우, 기존과 동일한 reconstruction loss와, Discriminator를 사용한 GAN loss로 해결이 가능합니다.</p>

\[\mathcal{L}_{\text{rec}}(E,G) = \mathbb{E}_{x \sim \mathbf{X}} [||x - G(E(x))||_1]\]

\[\mathcal{L}_{\text{GAN, rec}}(E,G,D) = \mathbb{E}_{x \sim \mathbf{X}} [-\text{log}(D(G(E(x))))]\]

<h3 id="latent-code-swapping">Latent Code Swapping</h3>

<p>Latent Code는 앞서 말했듯이 <strong>구조</strong>($z_s$) 와 <strong>텍스쳐</strong>($z_t$)로 나뉘며, 구조는 (효과적인 구조 인코딩을 위해) 2D latent space를, 텍스쳐는 1D latent space를 사용합니다.<br />
Encoder 상에서는 4개의 Downsampling ResNet 블록을 거친 후, convolution layer를 통해 $z_s$를 구하는 구간과, convolution layer 와 average pooling layer를 통해 $z_t$를 구하는 구간으로 나뉩니다.<br />
이런 구조를 통해 텍스쳐 ($z_t$)는 구조적 정보가 결여되어 거시적 텍스쳐 정보만을 인코딩하며, 구조 ($z_s$)는 좁은 receptive field를 통해 미시적 구조 정보를 인코딩하게 됩니다.</p>

<p>(256 * 256 해상도의 이미지가 있을 때, $z_s \in \mathbb{R}^{16 \times 16 \times 8}$이며 $z_t \in \mathbb{R}^{1 \times 1 \times 2048}$ 입니다.)</p>

<p>이때, 이렇게 분리된 latent code들이 여전히 진짜같은 이미지들을 생성할 수 있음을 입증하기 위해 GAN loss를 추가로 사용합니다.</p>

\[\mathcal{L}_{\text{GAN, swap}}(E,G,D) = \mathbb{E}_{x^1, x^2 \sim \mathbf{X}, x^1 \neq x^2} [-\text{log}(D(G(z^1_s, z^2_t)))]\]

<p>즉, (훈련 시 랜덤하게 뽑힌) 서로 다른 이미지 $x^1, x^2$ 와 거기서 나온 latent code $(z^1_s, z^1_t), (z^2_s, z^2_t)$들이 있을때,<br />
이미지1의 구조를 encoding한 $z^1_s$과, 이미지2의 텍스쳐를 encoding한 $z^2_t$를 Generator에 넣어도, 실제 같은 이미지가 생성되는지를 확인하는 Loss 입니다.</p>

<p>기존 GAN이나 VAE처럼 gaussian latent space를 추구하는 다른 논문들과는 달리, 저자는 <strong>latent space의 분포에 어떠한 정해진 분포를 사용하지 않으며</strong>, 다만 개별적인 이미지들의 수정이 말이 되게 이루어지는지만 확인합니다.<br />
(그러나 필자는 간단한 오토인코더 구조를 쓰는 상황에서 이 말이 의미가 있는지, 그리고 StyleGAN 처럼 w space, s space 처럼 분포를 배우는 것과 비교했을 때 결정적인 이점이 있는지 의문이 듭니다.)</p>

<h3 id="patch-discriminator--패치간-텍스쳐-균일-여부">Patch Discriminator : 패치간 텍스쳐 균일 여부</h3>

<p>위의 방식대로 latent code를 나눌 순 있었지만, 나눠진 code들이 각각 structure와 texture를 encoding하기 위해서 저자들은 Patch Discriminator를 새로 추가한다.<br />
이 Discriminator의 전제는, <strong>텍스쳐가 같은 이미지들의 부분들은 (구조 정보가 누락되어 있으므로) 서로 유사해야 한다는 것</strong>에 있습니다.</p>

<p>이 전제를 위해 Discriminator는 텍스쳐의 base가 되는 원본 이미지 $x^2$와 그 이미지에서 텍스쳐를 뽑아 새로 만든 변조 이미지 ($G(z^1_s, z^2_t)$) 간의 이미지 패치들을 랜덤하게 선정해서, 그 패치들이 서로 같은 분포에 있는지를 확인합니다.<br />
이 때, 원본 이미지에서는 여러 장의 패치들을, 변조 이미지에서는 한 장의 패치를 랜덤하게 선택해서, 변조 이미지의 패치가 원본 패치들의 분포에 있는지를 확인하며, 전체 이미지의 크기에 비례해 1/16 ~ 1/64 크기가 되도록 패치를 뽑습니다.</p>

<p>이러한 텍스쳐에 대한 전제는 Julesz의 텍스쳐 인지론에 영향을 받았다고 저자들은 밝힙니다.</p>

\[\mathcal{L}_{\text{CooccurGAN}}(E,G,D_{\text{patch}}) = \mathbb{E}_{x^1, x^2 \sim \mathbf{X}} [-\text{log}(D_{\text{patch}}(\text{crop}(G(z^1_s, z^2_t)), \text{crops}(x^2)))]\]

<h2 id="실험-및-결과">실험 및 결과</h2>

<p>저자들은 얼굴 (FFHQ), 동물 (AFHQ), 건축물 (LSUN Churches, Bedrooms), 그림 (Portrait2FFHQ), 자연물 (Flickr Mountain, Waterfall) 등 <strong>다양한 데이터셋</strong>에 대해 실험을 실시했고,<br />
이 중 그림, 자연물 데이터셋 등은 직접 수집해서 제작했습니다.</p>

<p>비교군으로는 Image2StyleGAN, STROSS, WCT을 선정했습니다.</p>

<p><img src="/assets/images/swap_ae/reconstr.png" alt="reconstr" /></p>

<p>Image Embedding (이미지 $x$ 를 latent code $z$로 바꾸고 reconstruction이 얼마나 잘 되는지) 에서 Image2StyleGAN 과 LPIPS metric으로 비교했을 시,<br />
LSUN Church를 제외한 데이터들에서 높은 성적을 기록했고 AutoEncoder 구조상 속도도 1000배 가까이 빠른 모습을 보입니다.<br />
Image2StyleGAN과만 비교했기 때문에 SOTA 급이라고는 할 수 없지만, image reconstruction이 <strong>뿌옇지 않고 미시적인 구조들을 유지</strong>하고 있는 것과,<br />
빠른 속도로 Image Embedding / Reconstruction이 이루어짐을 볼 수 있습니다.</p>

<p><img src="/assets/images/swap_ae/swap2.jpeg" alt="swap2" /></p>

<p>두 이미지의 구조와 텍스쳐를 섞어서 이미지를 편집하는 Image Editing의 경우,<br />
저자들은 Amazon Mechanical Turker (이하 AMT) 들을 통해 설문조사를 실행해서<br />
Swapping AutoEncoder가 타 모델 대비 더 자연스러운 편집을 해준다는 결과를 확인할 수 있었습니다.<br />
또 놀라운건 별도의 semantic input이 없이도 하늘, 건물 벽, 지면 등의 요소들이 <strong>일관되게 텍스쳐가 바뀌는</strong> 모습을 확인할 수 있었습니다.<br />
(하늘의 색깔이 건물에 스며들거나, 건물 텍스쳐가 땅에 입혀지는 일이 거의 없이 알맞게 들어갔습니다.)</p>

<p><img src="/assets/images/swap_ae/interp1.gif" alt="interp1" />
<img src="/assets/images/swap_ae/edit1.gif" alt="editing" /></p>

<p>Latent code 에 대한 interpolation을 통한 image translation / editing도 부드럽게 되었습니다.<br />
가령 그림을 사진으로 바꾸는 태스크에서 저자들은 그림과 사진을 같이 훈련 시킨 후, 그림에 해당하는 데이터들의 latent code와 사진 latent code들의 차이를 계산해서, 그 차이만큼 입력에 적용사키는 걸로 그림에서 사진으로의 image translation을 실행합니다.<br />
이런 비교적 단순한 코드 적용으로도 자연스러운 image translation이 가능할 뿐만 아니라, 2D 구조 latent code 위에 <strong>그림판으로 그리듯 수정</strong>하는 식으로 image editing도 수행할 수 있음을 보여줍니다.
(사전에 PCA로 뽑은 latent vector들을 추가하는 식으로)</p>

<h2 id="정리">정리</h2>

<p>Swapping AutoEncoder는 Image Editing task에 있어서 <strong>굳이 Unconditional Image Synthesis –&gt; GAN Inversion을 거칠 필요가 없으며</strong>, 
AutoEncoder 구조로도 효과적인 Image Editing이 가능하다는 것을 보여줍니다.</p>]]></content><author><name></name></author><category term="blog" /><category term="image-editing" /><summary type="html"><![CDATA[논문스터디 : Swapping Autoencoder for Deep Image Manipulation]]></summary></entry><entry><title type="html">E4E - Encoder for StyleGAN Image Manipulation</title><link href="https://yj7082126.github.io/blog/2022/Encoder4Editing/" rel="alternate" type="text/html" title="E4E - Encoder for StyleGAN Image Manipulation" /><published>2022-02-20T19:10:00+00:00</published><updated>2022-02-20T19:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2022/Encoder4Editing</id><content type="html" xml:base="https://yj7082126.github.io/blog/2022/Encoder4Editing/"><![CDATA[<h2 id="논문스터디--e4e-designing-an-encoder-for-stylegan-image-manipulation">논문스터디 : E4E (Designing an Encoder for StyleGAN Image Manipulation)</h2>

<table>
  <thead>
    <tr>
      <th>이름</th>
      <th><strong>Designing an Encoder for StyleGAN Image Manipulation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>저자</td>
      <td><strong>Omer Tov</strong>, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or</td>
    </tr>
    <tr>
      <td>토픽</td>
      <td>StyleGAN, GAN-Inversion</td>
    </tr>
    <tr>
      <td>년도</td>
      <td>2021.02</td>
    </tr>
    <tr>
      <td>학회</td>
      <td>SIGGRAPH 2021</td>
    </tr>
    <tr>
      <td>링크</td>
      <td><a href="https://arxiv.org/abs/2102.02766">[논문]</a>, <a href="https://github.com/omertov/encoder4editing">[코드]</a></td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/e4e/teaser.jpeg" alt="Encoder4Editing 예시" /></p>

<h2 id="introduction">Introduction</h2>

<p>GAN 이후로 발표된 여러 이미지 생성모델 중에서도 StyleGAN이 두각을 보인 까닭은, 단순히 생성된 이미지의 질이 좋다는 이유 뿐만이 아닌, 학습을 통해 배운 $\mathcal{W}$ latent space를 사용하는데 있습니다.</p>

<p>StyleGAN (1,2,3 모두)는 가우시안 latent space $\mathcal{Z}$에서 이미지 분포에 가깝도록 훈련된 latent space $\mathcal{W}$ 로 변형시켜주는 <em>mapping network</em>와, $w \in \mathcal{W}$ 에서 이미지를 생성하는, 점진적으로 출력 해상도가 증가하는 형태의 네트워크인 <em>synthesis network</em>로 나뉩니다. (이 때, $w$는 synthetic network의 각 레이어마다 들어가는 style vector로 구성되어, $w$의 dimensionality는 $w \in \mathbb{R}^{\text{레이어 수} \times 512}$ 입니다.)</p>

<p>여기서 $\mathcal{W}$ latent space는 다른 모델들의 비교적 단순한 분포들 (예 : 가우시안) 가진 latent space과는 달리, 이미지의 실분포를 보다 잘 설명해주며 latent space 상의 요소들이 실제 이미지 요소에 따라 잘 나뉘어져 있어 (예 : 얼굴에 대한 StyleGAN의 경우 표정, 머리카락, 나이 등) 편집이 용이하다는 장점이 있습니다.</p>

<p>그러나 이런 StyleGAN의 장점을 유지한 채로 실제 이미지에 편집을 하기 위해선, 그 이미지에 대응하는 StyleGAN에서의 latent code $w \in \mathcal{W}$ 를 찾아야 합니다. 이를 <em>GAN-Inversion</em>이라고 부릅니다. 이미지에 맞는 $w$를 찾은 후에는 latent space의 성질을 사용해 $w$를 조작해, 이미지를 목표에 따라 편집합니다. 이를 <em>latent-space manipulation</em>이라고 부릅니다.</p>

<p>보통 $w$를 구하는 데 사용하는 두가지 방법인 <em>learning-based GAN Inversion</em> (StyleGAN과는 별도의 Encoder network를 사용해서 $w$를 계산) 과 <em>optimization-based GAN Inversion</em> (별도의 네트워크 없이 $w$ 자체를 최적화해서 계산) 중, 저자들은 전자에 집중하고 있습니다.</p>

<h2 id="distortion-editability-distortion-perception-tradeoffs">Distortion-Editability, Distortion-Perception Tradeoffs</h2>

<p>GAN-Inversion을 통해 구한 $w$는 <em>1. StyleGAN에 넣었을 때 생성될 이미지가 원래 이미지와 최대한 같아야 하고 (재구성 능력, reconstruction), 2. latent space manipulation 편집을 용이하게 하기 위해 latent space 상에서 분리된 요소를 잘 따라야 합니다. (편집 능력, editability)</em> 아무리 재구성 능력이 좋아 원래 이미지와 똑같은 이미지를 만들 수 있어도, 특정 목적에 따른 편집이 되지 않으면 이미지 편집을 위해 StyleGAN을 사용하는 의미가 없습니다. 마찬가지로, 아무리 편집 능력이 좋아도 기본적인 이미지 재구성이 되지 않아 원래 이미지처럼 만들 수 없는 경우, 역시 StyleGAN을 사용하는 이미지가 없습니다.</p>

<p>여기서 전자 (재구성)의 경우, 2가지로 나뉘어서 <em>원 이미지와 생성된 이미지 간의 차이</em>를 <strong>distortion</strong>, <em>생성된 이미지가 얼마나 실제 이미지 같은지</em>를 <strong>perceptual quality</strong>로 정의합니다. 언듯 같은 목표에 대한 개념들로 보이지만, 다음의 예시처럼 두 개념은 상충될 수 있기 때문에, 나누어서 생각하는 것이 중요합니다.</p>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/e4e/figure5.png" alt="figure5" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>distortion과 perceptual quality간의 trade-off 예시. 가운데 이미지는 입력 이미지 (왼쪽)와 낮은 픽셀 차이 (낮은 distortion)을 보이지만, 말의 사진인데도 눈이나 사람들이 “말”하면 생각날 기본적인 요소들이 결락되어, 낮은 perceptual quality를 보입니다. 반면 오른쪽 이미지는 더 높은 distortion을 보이지만, 실제 말을 찍은 것과 같은 높은 perceptual quality를 보입니다.</td>
    </tr>
  </tbody>
</table>

<p>저자들은 이 <strong>3가지 요소 사이에 불균형이 존재</strong>하며, 그 균형을 맞추기는 어렵다고 주장합니다.</p>

<p>가령 Image2StyleGAN의 경우, 이미지 재구성 능력을 키우기 위해 $\mathcal{W}$의 확장 영역인 $\mathcal{W}+$을 사용하여, 굉장히 낮은 distortion을 보여줍니다. 그러나, 그 과정에서 이미지 편집이 용이하지 않은 latent space 영역으로 $w$의 분포가 이동하면서 이미지 편집이 부자연스럽게 되며, perceptual quality또한 손상되는 문제가 발생합니다.</p>

<h2 id="e4e--encoder-for-editing">E4E : Encoder for Editing.</h2>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/e4e/figure6.png" alt="figure5" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>E4E의 전반적인 구성. $\mathcal{W}+$ 차원의 Encoder 모델은 이미지를 입력받아, 단일 style vector $w$와, offset $\Delta_1, …, \Delta_{N-1}$ 를 output으로 출력하고, $w$를 N번 반복해서 offset들을 더하는걸로 최종 inversion latent code를 완성합니다.</td>
    </tr>
  </tbody>
</table>

<p>이렇게 distortion을 낮추기 위해 perceptual quality와 editability를 희생해야 하는 상황을 피하기 위해, 저자들은 $w$는 최대한 $\mathcal{W}$ 분포와 ‘가까워’야 한다고 주장합니다. 여기서 ‘가깝다’라는 것은, <em>$w$를 구성하는 각 style vector간 variance가 낮으면서</em>, 그 style vector들이 <em>$\mathcal{W}+$ 가 아닌 $\mathcal{W}$의 분포에 있음</em>을 뜻합니다.</p>

<p>이 법칙에 대해 훈련되어 이미지 재구성 및 편집간 균형을 맞추어, 기존보다 뛰어난 성능을 보이는 Encoder network를 저자들은 <em>Encoder for Editing - 이하 e4e</em>라고 부릅니다. 모델의 기본적인 구성은 pSp와 유사하지만, 훈련 방식 및 로스 등에서 e4e는 pSp와 큰 차이를 보입니다.</p>

<p>우선 style vector간 variance를 낮추기 위해, 저자들은 새로운 <em>“progressive training”</em> 방식을 제안합니다. 기존의 encoder-based optimization 방법들은 모두, 이미지 입력 $x$를 받아 $w_0, w_1, …$의 서로 다른 style vector들을 생성하는 방식이었고, 이는 style vector들 간의 분포가 지나치게 넓어지는 원인이 되었습니다.</p>

\[E(x) = w \quad (w = (w_0, w_1, ..., w_{N-1}), w \in \mathbb{R}^{N\times512})\]

<p>반대로 e4e는, 한 개의 style vector인 $w_0 \in \mathbb{R}^{512}$와, $w_0$에 더할 (레이어 수-1 개의) <strong>offset</strong> 들을 예측하고, 이 offset들을 각각 적용하는 방식으로 $w$를 생성합니다.</p>

<p>\(E(x) = (w_0, \Delta) \quad (\Delta = (\Delta_1, ..., \Delta_{N-1}), \Delta \in \mathbb{R}^{(N-1)\times512})\)
\(w = (w_0, w_0 + \Delta_1, ..., w_0 + \Delta_{N-1})\)</p>

<p>훈련 시작 시, offset들은 $\forall i : \Delta_i = 0$로 초기화하고 encoder는 $w_0$ 만을 배우도록 훈련한 뒤, 일정 수준 이상의 이미지 재구성 능력이 확보되었을 때, 각 i에 대한 $\Delta_i$를 <em>순차적으로</em> (한 차례에 하나씩) 배웁니다. 이를 통해 모델을 초기 레이어의 거시적 이미지 구조부터 시작하여 후기 레이어의 미시적 이미지 요소들을 완성시키는 offset들을 배워 생성된 이미지의 질을 높이면서, style vector들간 variance가 높지 않도록 조절합니다. 특히 variance의 경우, 명시적으로 조절하기 위해 다음의 loss term을 사용합니다.</p>

\[\mathcal{L}_{d-reg}(w) = \sum^{N-1}_{i=1} || \Delta_i ||_2\]

<p>여기에 더해 style vector들이 $\mathcal{W}$의 분포안에 있게끔 하기 위해 Discriminator를 사용, $\mathcal{W}$의 분포에서 뽑은 ‘진짜’ $w$ 들과 Encoder에서 나온 ‘가짜’ $w$들간 구별을 하도록 합니다. 이 Discriminator를 통해 직접적으로 정의 내릴수 없는 $\mathcal{W}$ 공간에 대해 작업을 할 수 있습니다. Loss는, R1 Regularization을 더한 Non-saturating GAN loss를 사용합니다.</p>

<p>\(\mathcal{L}^{D}_{adv} = - \mathbb{E}_{w \sim \mathcal{W}}[\text{log }D_{\mathcal{W}}(w)] - \mathbb{E}_{x \sim p_x}[\text{log }(1 - D_{\mathcal{W}}(E(x)_i))] + \frac{\gamma}{2}\mathbb{E}_{w \sim \mathcal{W}}[||\nabla_wD_{\mathcal{W}}(w)||^2_2]\)
\(\mathcal{L}^{E}_{adv} = -\mathbb{E}_{x \sim p_x}[\text{log }D_{\mathcal{W}}(E(x)_i)]\)</p>

<p>이렇게 $w$ 간 variance를 억제하는 $\mathcal{L}<em>{d-reg}$와, $\mathcal{W}$의 분포에 오게끔 하는 $\mathcal{L}</em>{adv}$ 로스를 합쳐서, perceptual quality와 editability를 보존하는 $\mathcal{L}_{edit}$을 사용합니다.</p>

\[\mathcal{L}_{edit} = \lambda_{d-reg}\mathcal{L}_{d-reg} + \lambda_{adv}\mathcal{L}_{adv}\]

<p>한편, distortion (과 perceptual quality의 강화)를 위해선, MoCov2 로 훈련된 ResNet50 네트워크 ($C$)를 사용, 실제 사진과 Encoder를 통해 구한 latent code를 재구성한 가짜 사진들의 ResNet50 feature들간 cosine similarity를 최소화 시키는 loss term을 사용합니다. (대상이 사람의 얼굴일 경우, ArcFace facial recognition network를 사용합니다.)</p>

\[\mathcal{L}_{sim} = 1 - &lt;C(x), C(G(E(x)))&gt;\]

<p>여기에 L2 loss, LPIPS loss 등 distortion / perception quality를 강화하기 위한 로스들을 사용해서, 최종적으로 다음과 같이 로스를 완성시킵니다.</p>

\[\mathcal{L}_{dist} = \lambda_{l2}\mathcal{L}_{l2} + \lambda_{lpips}\mathcal{L}_{lpips} + \lambda_{sim}\mathcal{L}_{sim}\]

\[\mathcal{L} = \mathcal{L}_{dist} + \lambda_{edit}\lambda_{edit}\]]]></content><author><name></name></author><category term="blog" /><category term="stylegan" /><category term="gan-inversion" /><summary type="html"><![CDATA[논문스터디 : E4E (Designing an Encoder for StyleGAN Image Manipulation)]]></summary></entry><entry><title type="html">LaMa - Large Mask Inpainting</title><link href="https://yj7082126.github.io/blog/2022/LaMa/" rel="alternate" type="text/html" title="LaMa - Large Mask Inpainting" /><published>2022-01-30T19:10:00+00:00</published><updated>2022-01-30T19:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2022/LaMa</id><content type="html" xml:base="https://yj7082126.github.io/blog/2022/LaMa/"><![CDATA[<h2 id="lama-resolution-robust-large-mask-inpainting-with-fourier-convolutions">LaMa (Resolution-robust Large Mask Inpainting with Fourier Convolutions)</h2>

<table>
  <thead>
    <tr>
      <th>이름</th>
      <th><strong>LaMa : Resolution-robust Large Mask Inpainting with Fourier Convolutions</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>저자</td>
      <td>Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha,  Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky</td>
    </tr>
    <tr>
      <td>토픽</td>
      <td>Image Inpainting</td>
    </tr>
    <tr>
      <td>년도</td>
      <td>2021.09</td>
    </tr>
    <tr>
      <td>학회</td>
      <td> </td>
    </tr>
    <tr>
      <td>링크</td>
      <td><a href="https://arxiv.org/abs/2109.07161">[논문]</a>, <a href="https://github.com/saic-mdal/lama">[코드]</a></td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/lama/lama-1.gif" alt="Intro" /></p>

<h3 id="intro">Intro</h3>

<p><strong>이미지 인페인팅 (주어진 이미지와 (유저가 입력한) ‘마스크’가 있을때, 마스크로 가려진 부분들을 자연스럽게 유추하는 문제)</strong> 는 이미지 생성뿐만이 아닌, <strong>거시적인 이미지 구조를 ‘이해’하는</strong> 것을 요구합니다.</p>

<p>현재 GAN을 통한 이미지 생성이 실제와 분간이 되지 않을 정도록 뛰어난 성능을 보임에도 불구하고, 아직 이미지 인페인팅 문제는 이미지의 대부분을 덮는 마스크나 복잡한 구조의 마스크, 고해상도의 이미지 등의 상황에 대해 제대로 대처를 하지 못하고 있으며, 본 논문의 저자들은 이를 <strong>small receptive field의 문제</strong> 라고 보고 있습니다.</p>

<p>이를 해결하기 위해, 저자들은 LaMa(Large Mask Inpainting) 모델을 소개합니다. LaMa는 <strong>Fast Fourier Convolution</strong> 을 기반으로 하여 모델의 초기 단계에서 부터 이미지 전체를 덮을수 있는 Receptive Field를 가지고 있으며, loss function 도 <strong>High Receptive Field를 기반으로한 Perceptual loss를</strong> 사용하여 이미지의 거시적인 구조를 수월하게 이해할 수 있습니다. Fast Fourier Convolution의 특성상, <strong>고해상도의 이미지에 대해서도 문제없이 적응이</strong> 가능합니다.
또한 훈련 단계에서 크고 복잡한 모양의 마스크들을 사용해서 해당 마스크들에 대한 저항력을 기릅니다.</p>

<p>저자들은 연구를 통해, 256해상도에서만 훈련시킨 본 모델로도 고해상도의 이미지와 큰 마스크들에 대해서 자연스럽게 이미지 인페인팅을 수행할 수 있음을 보여줍니다.</p>

<hr />

<h3 id="main-reference-works">Main Reference Works</h3>
<ul>
  <li>[4] : <a href="https://github.com/pkumivision/FFC">Fast fourier convolution</a> (<a href="https://arxiv.org/abs/2010.04257">논문</a>)</li>
  <li>[29] : <a href="https://arxiv.org/abs/1701.04128">Understanding the effective receptive field in deep convolutional neural networks</a></li>
</ul>

<hr />

<h3 id="method">Method</h3>
<p>이미지가 x, 마스크가 m일때, 모델 f(변수: $\theta$)는 마스크로 덮혀진 이미지와 마스크를 입력으로 받아, 마스크 부분이 인페인팅된 이미지를 받습니다.</p>

\[\hat{x}=f_{\theta}(\text{stack}(x \odot m,m))\]

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-2.png" alt="FFC구조" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FFC 모델의 전반적인 구조. Local (미시적 요소) / Global (거시적 요소) 로 나뉜 feature들 중에서 Local은 고전적인 convolution을, Global은 Fourier Transformation을 통해 얻은 주파수에서의 convolution을 통해 모델의 이미지 전체에 대한 이해를 돕습니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="fast-fourier-convolution-ffc">Fast Fourier Convolution (FFC)</h3>
<p>ResNet과 같은 기존 convolution 기반 모델들은, <strong>작은 사이즈의 커널 (3*3 사이즈)</strong> 때문에 유효한 receptive field가 초반에 작아서 이미지 전체를 덮지 못하며, 커지는 속도도 느립니다. 따라서 모델 내부 대다수의 레이어들은 거시적인 구조에 대한 정보를 담지 못하고, 인페인팅시 썩 자연스럽지 못한 결과가 나옵니다. 특히 면적이 큰 마스크들의 경우, <strong>receptive field 전체가 마스크 안에 있게 되는 불상사</strong>가 발생하여 인페인팅이 불가능한 경우도 발생합니다.</p>

<p>이를 막기 위해 초기 단계에서 거시적인 구조에 대한 정보를 담을 수 있는 레이어가 필요하고, 이를 위해 <strong>FFC (Fast Fourier Convolution)이</strong> 필요합니다.</p>
<ul>
  <li>FFC는 채널 단위 <strong>Fast Fourier Transform (FFT)</strong> 를 사용하며, 이를 통해 이미지 전체를 덮을 수 있는 receptive field를 가집니다.</li>
  <li>FFC는 두 가지 갈래의 연산을 수행하는데, <em>local branch</em>는 미시적인 구조를 위한 고전적인(?) convolutional layer를, <em>global branch</em>는 거시적인 구조를 위한 real FFT를 사용합니다. 이후 두 갈래의 출력값을 합쳐서 결과를 출력합니다.</li>
  <li>FFC는 미분가능하며, convolutional layer에 1:1로 대응이 가능합니다.
    <ul>
      <li>초기 단계부터 이미지 전체를 이해하는 네트워크를 생성 가능하며, 이를 통해 효율적인 훈련이 가능합니다.</li>
    </ul>
  </li>
  <li>또한 receptive field가 이미지 전체를 덮을 수 있기 때문에, 고해상도 이미지에 효과적으로 적용가능합니다.</li>
  <li>결과를 확인해보면 벽돌, 사다리, 창문 등 <strong>인공적인 구조물 패턴에 잘 대응하는걸 볼 수 있습니다.</strong></li>
</ul>

<p>FFC 중에서도 Real FFT의 상세 구조는 다음과 같습니다;</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ifft_shape_slice</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># RealFFT2d : Real(batch, c, h, w) --&gt; Complex(batch, c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="nf">rfftn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="sh">'</span><span class="s">ortho</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># ComplexToReal : Complex(batch, c, h, w/2) --&gt; Real(batch, 2c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">ffted</span><span class="p">.</span><span class="n">real</span><span class="p">,</span> <span class="n">ffted</span><span class="p">.</span><span class="n">imag</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">:])</span>

<span class="c1"># Convolutional Block : Real(batch, 2c, h, w/2) --&gt; Real(batch, 2c, h, w/2)
# 1*1 kernel이지만, 위 fft에서 변환된 이미지의 주파수 도메인에 대해 작업하므로, 
# 이미지 전역을 덮는 receptive field를 가집니다.
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv_layer</span><span class="p">(</span><span class="n">ffted</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">ffted</span><span class="p">)</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">sel</span>
<span class="c1"># RealToComplex : Real(batch, 2c, h, w/2) --&gt; Complex(batch, c, h, w/2)
</span><span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:])</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">ffted</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">ffted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">complex</span><span class="p">(</span><span class="n">ffted</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">ffted</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># InverseRealFFT2d : Complex(batch, c, h, w/2) --&gt; Real(batch, c, h, w)
</span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="nf">irfftn</span><span class="p">(</span><span class="n">ffted</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">ifft_shape_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="sh">'</span><span class="s">ortho</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-3.png" alt="모델구조" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>모델 전체의 구조. 비교적 간단한 모래시계 모양의 구조를 가집니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="모델-전반-구조">모델 전반 구조</h3>
<ul>
  <li>입력 : 마스크로 덮힌 이미지 + 마스크 (크기 : 4<em>해상도</em>해상도)</li>
  <li>출력 : 마스크로 덮힌 부분이 칠해진 이미지 (크기 : 3<em>해상도</em>해상도)</li>
  <li>모델 : 모래시계 모양의 ResNet-structure 모델
    <ol>
      <li>Reflection Padding + Conv-BatchNorm-ReLU Block (ksize=7)</li>
      <li>Downsampling Blocks * 3
        <ul>
          <li>Conv-BatchNorm-ReLU Block (ksize=3, stride=2, padding=1)</li>
          <li>마지막 블록에서 feature를 local과 global로 나눔
            <ul>
              <li>이때, <strong>local은 채널중 25%, global은 75%의 비율로 나눕니다.</strong></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>FFC Residual Blocks * 9 (big-LAMA 세팅이면 18)
        <ul>
          <li>마지막 블록에서 나눠진 local과 global을 다시 하나의 feature로 합침</li>
        </ul>
      </li>
      <li>Upsampling Blocks   * 3
        <ul>
          <li>ConvT-BatchNorm-ReLU Block (ksize=3, stride=2, padding=1)</li>
        </ul>
      </li>
      <li>Reflection Padding + Conv-BatchNorm-ReLU Block (ksize=7) + Sigmoid</li>
    </ol>
  </li>
</ul>

<hr />

<h3 id="손실함수">손실함수</h3>
<p>인페인팅 함수에서 손실함수를 정하는건, 가능한 출력물의 가짓수 때문에 매우 어려운 일이 됩니다.</p>

<p>이를 위해 저자들은 Perceptual Loss 중에서도 특별한 형태의 손실함수를 사용합니다.</p>
<ul>
  <li>기존의 supervised loss의 경우, 마스크가 이미지 대부분을 덮고 있을때 모델의 학습을 어렵게 하기 때문에 출력물의 해상도가 떨어져서 나오는 경우가 많았습니다.</li>
  <li>이에 비해 <strong>perceptual loss</strong>는 사전에 훈련된 네트워크 (ResNet50 기반)를 통해 이미지간의 feature distance를 측정하기 때문에, <strong>큰 마스크에도 상대적으로 잘 되며 다양한 출력물을 지원합니다.</strong></li>
  <li>Perceptual Loss에서 쓰일 네트워크도 초반 레이어의 넓은 receptive field와, 이를 통한 거시적인 이미지 구조에 대한 이해가 필요하기 때문에, 저자들은 <strong>High Receptive Field Model (이하 HRFM)을</strong> 새로 설계하여 perceptual loss를 구합니다.
    <ul>
      <li>Fourier / Dilated convolution을 기존 convolution 대신 사용</li>
    </ul>
  </li>
  <li>해당 HRFM이 어떻게 훈련되었는지도 중요합니다.
    <ul>
      <li>Segmentation-based 일 경우, 배경의 사물같은 고레벨 구성요소 정보에 대해 집중할 수 있습니다.</li>
      <li>Classification-based일 경우, 배경 텍스쳐같은 거시적 정보에 집중할 수 있습니다.</li>
      <li>선택과 집중의 문제</li>
    </ul>
  </li>
</ul>

<p>또한 <a href="https://github.com/phillipi/pix2pix">Pix2Pix</a>에 영향을 받은 Discriminator를 추가해서 <strong>Adversarial Loss를</strong> 손실함수에 포함시킵니다.</p>
<ul>
  <li>해당 Discriminator는 <strong>local patch 단계에서 계산해, 해당 patch가 마스크로 덮힌 부분인지 아닌지를 분별합니다.</strong></li>
  <li>Adversarial Loss는 <strong>Non-saturating adversarial loss의</strong> 형태로 사용됩니다.</li>
  <li>또한, Discriminator에 대한 feature-matching loss를 추가로 계산해, GAN 훈련의 안정성을 늘립니다.</li>
  <li>이 두 손실함수를 통해 자연스럽게 원본 이미지와 맞물리는 이미지를 생성할 수 있습니다.</li>
</ul>

<p>마지막으로 R1 gradient penalty 함수를 추가하는걸로 손실함수를 완성시킵니다.</p>

<hr />

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-8.png" alt="데이터셋 예시" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>훈련용 데이터셋 마스크 생성 예시. LaMa는 밑의 두 행처럼 마스크를 생성합니다. 보시는 것처럼 기존보다 더 크고 다양한 면적의 마스크들을 볼 수 있습니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="데이터셋-마스크-생성">데이터셋 마스크 생성</h3>

<p>다양한 형태의 입력 마스크에 성공적으로 대응하기 위해서는 훈련 단계에서 다양한 마스크를 생성해서 같이 훈련시키는 것이 중요합니다.</p>
<ul>
  <li>저자들은 이를 위해 기존보다 <strong>더 크고 다양한 면적을 덮는 마스크 생성법을</strong> 사용합니다.</li>
  <li>이를 위해 두 가지 방식으로 마스크를 생성하는데, 하나는 polygonal chain에서 크기를 더 키운 wide mask 방식이고, 다른 하나는 임의의 비율을 가진 직사각형을 사용한 box mask 방식입니다.</li>
  <li>이때, 마스크가 이미지의 50% 이상을 넘지 않도록 합니다.</li>
</ul>

<hr />

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-4.png" alt="실험결과" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>서로 다른 데이터셋과 마스크 종류에 따른 인페인팅 모델들의 결과 비교. LaMa는 거의 모든 결과에 대해 타 모델 대비 우위를 차지하는걸 확인할 수 있습니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="실험-및-결과">실험 및 결과</h3>

<p>LaMa 모델은 실험에서 모델 크기가 몇배는 큰 모델들과의 비교에서도 LPIPS / FID metric면에서 훨씬 좋은 결과를 보여줍니다.</p>
<ul>
  <li>비교 실험에서 metric면에서 (부분적으로) LaMa 보다 나은 모델들이었던 <a href="https://github.com/zsyzzsoft/co-mod-gan">CoModGAN</a>과 <a href="https://github.com/MADF-inpainting/Pytorch-MADF">MADF</a>는 LaMA 대비 3~4배는 더 큰 모습을 보이며, segmentation mask를 대상으로 한 실험에서는 LaMa 보다 쳐지는 모습을 보입니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-5.png" alt="FFC 유효성" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>(훈련에 사용한 입력 해상도보다 큰 해상도의) 고화질 이미지를 대상으로한 인페인팅. FFC를 사용한 LaMa 외의 다른 모델들은 모두 인페인팅 결과가 부정확한 걸 확인할 수 있습니다.</td>
    </tr>
  </tbody>
</table>

<p>Ablation Study 결과;</p>
<ul>
  <li>FFC의 유무는 특히 고화질 이미지를 대상으로한 inference에서 눈에 띄며, 창문이나 철조망 같은 반복적 구조의 inpainting에 큰 영향을 끼칩니다.
    <ul>
      <li>Dilated Convolution의 경우, FFC와 비슷한 성질을 가지며 (어느 정도는) 대용으로 사용이 가능하지만, <strong>receptive field가 더 제한이 되어</strong> 고화질 이미지를 대상으로 적용이 어렵다는 단점을 가집니다.</li>
    </ul>
  </li>
  <li>Perceptual Loss의 기본이 되는 모델의 선정은 최종 결과에 중요하게 적용됩니다.
    <ul>
      <li>논문에서는 <strong>ResNet50 기반 Segmentation + ADE20K</strong> 데이터셋 훈련 모델을 채택했습니다.</li>
    </ul>
  </li>
</ul>

<hr />

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/lama/lama-7.png" alt="부정적 lama 예시" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>부정적인 결과 예시. 인물을 지운 부분에 흐릿한 artifact가 생기는걸 볼 수 있으며, 이는 데이터셋 분포에 포함되어 있지 않던 복잡한 배경 및 마스크 크기 때문입니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="결론">결론</h3>

<p>저자들은 FFC 레이어와 특수 perceptual loss, 마스크 데이터의 생성을 통해 단순한 모델구조로도 다양한 상황에 쉽게 사용될수 있는 image inpainting 모델을 제시했습니다.</p>

<p>아직 나름대로의 한계 (데이터셋 분포에 있지 않은 이미지들에 대한 인페인팅, 사람 몸같은 큰 대상의 인페인팅시 왜곡 현상)등이 있지만, 현재까지 나온 모델들 중에서 가장 자연스럽게 인페인팅이 가능한 모델이라고 생각됩니다.</p>

<p>저자들은 또한 FFC나 Dilated Convolution 뿐만이 아닌, Vision Transformer의 사용도 염두에 두면서 향후 이미지 인페인팅 연구에 대한 기대감을 심어줍니다.</p>]]></content><author><name></name></author><category term="blog" /><category term="image inpainting" /><summary type="html"><![CDATA[LaMa (Resolution-robust Large Mask Inpainting with Fourier Convolutions)]]></summary></entry><entry><title type="html">Hyperstyle - StyleGAN Inversion with HyperNetworks for Real Image Editing</title><link href="https://yj7082126.github.io/blog/2022/HyperStyle/" rel="alternate" type="text/html" title="Hyperstyle - StyleGAN Inversion with HyperNetworks for Real Image Editing" /><published>2022-01-26T19:10:00+00:00</published><updated>2022-01-26T19:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2022/HyperStyle</id><content type="html" xml:base="https://yj7082126.github.io/blog/2022/HyperStyle/"><![CDATA[<h2 id="hyperstyle-stylegan-inversion-with-hypernetworks-for-real-image-editing">Hyperstyle (StyleGAN Inversion with HyperNetworks for Real Image Editing)</h2>

<table>
  <thead>
    <tr>
      <th>이름</th>
      <th><strong>HyperStyle : StyleGAN Inversion with HyperNetworks for Real Image Editing</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>저자</td>
      <td><strong>Yuval Alauf</strong>, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano</td>
    </tr>
    <tr>
      <td>토픽</td>
      <td>StyleGAN, GAN-Inversion</td>
    </tr>
    <tr>
      <td>년도</td>
      <td>2021.11</td>
    </tr>
    <tr>
      <td>학회</td>
      <td> </td>
    </tr>
    <tr>
      <td>링크</td>
      <td><a href="https://arxiv.org/abs/2111.15666">[논문]</a>, <a href="https://github.com/yuval-alaluf/hyperstyle">[코드]</a></td>
    </tr>
  </tbody>
</table>

<p><img src="https://www.linuxadictos.com/wp-content/uploads/HyperStyle-1.jpg" alt="실사 얼굴 도메인에서의 이미지 복원과 수정." /></p>

<h3 id="intro">Intro</h3>
<p>GAN Inversion을 골치 아프게 하는 문제는, <strong>입력 이미지의 복원과 수정간의 균형</strong>을 맞추기 매우 어렵기 때문입니다; 이미지 복원을 중시하며 GAN Inversion을 실행시 사용되는 latent space region들은, 의미론적 수정을 하기 적합한 공간들이 아닌 경우가 매우 많습니다. (출처: <a href="https://github.com/ZPdesu/II2S">II2s</a>)</p>

<p>기존 논문들의 경우, 단일 latent vector를 직접적으로 최적화하는 경우 (예 : <a href="https://github.com/zaidbhat1234/Image2StyleGAN">Image2StyleGAN</a>) 이미지 복원에서 탁월한 성능을 보이나 수정이 잘 되지 않으며 단일 이미지에 몇분 이상의 시간을 소요하기 때문에 실용적이지 못합니다. 반면 별개로 훈련시킨 Encoder 기반 모델들은 (예 : <a href="https://github.com/eladrich/pixel2style2pixel">pSp</a>) 더 효율적이나 마찬가지로 이미지 복원과 수정간의 간극을 좁히지 못하는 모습을 보입니다.</p>

<p>최근에 <a href="https://github.com/danielroich/PTI">PTI</a> 는 기존의 latent vector optimization 대신, StyleGAN 모델을 이미지에 대해 finetune 하여 수정이 용이한 latent vector space로 이미지가 해석되도록 해서 복원/수정간 균형을 맞추었으나, 이 방법도 단일 이미지에 1분가량 걸리기 때문에 <strong>실용적이진 못합니다</strong>.</p>

<p>HyperStyle의 저자들은 바로 이 <a href="https://github.com/danielroich/PTI">PTI</a>의 구조를 Encoder-based 방법과 합쳐, StyleGAN모델의 weight 값을 배우는 hypernetwork를 훈련시켜 복원/수정간 균형 및 실용성도 잡는 모델을 소개합니다. 단순하게 생각하면 StyleGAN (2 기준) 의 모델 변수 숫자는 3000만개가 넘기 때문에, 저자들은 네트워크 디자인을 통해 Encoder 모델을 효율적으로 디자인해서 모델의 실용성을 보장하고, image translation (도메인 변경)을 비롯한 다양한 태스크에 적용될 수 있음을 보여줍니다.</p>

<hr />

<h3 id="previous-works">Previous Works:</h3>
<ul>
  <li>[33] : <a href="https://github.com/NVlabs/stylegan2-ada-pytorch">StyleGAN2-ada</a> (<a href="https://arxiv.org/abs/2006.06676">논문</a>)
    <ul>
      <li>지속적으로 커지는 크기의 conv.layer를 통해 이미지를 생성하는 모델 시리즈로, 특유의 latent space와 expressiveness로 이미지 생성 및 GAN inversion에 대표적으로 쓰이는 모델입니다. 본 논문에서는 StyleGAN2 (ada도 포함) 구조를 전제로 설명하고 있습니다.</li>
    </ul>
  </li>
  <li>[68] : <a href="https://github.com/omertov/encoder4editing">encoder4editing (aka e4e)</a> (<a href="https://arxiv.org/abs/2102.02766">논문</a>)
    <ul>
      <li>이미지 수정에 용이한 encoder-based GAN inversion 모델이며, 준 SOTA급 이미지 복원 능력과 상대적으로 좋은 수정 능력을 가지고 있습니다. 본 논문에서는 latent vector의 생성시 해당 모델을 사용합니다.</li>
    </ul>
  </li>
  <li>[5] : <a href="https://github.com/yuval-alaluf/restyle-encoder">Restyle</a> (<a href="https://arxiv.org/abs/2104.02699">논문</a>)
    <ul>
      <li>순차적으로 이미지에 대한 latent code optimization을 실행하는 encoder-based GAN inversion 모델이며, 위의 e4e 같은 encoder를 베이스로 사용, 이미지 복원 능력에 SOTA급 능력을 보여줍니다. 본 논문에서도 해당 순차적 구성을 사용합니다.</li>
    </ul>
  </li>
  <li>[23] : <a href="https://github.com/g1910/HyperNetworks">HyperNetworks</a> (<a href="https://arxiv.org/abs/1609.09106">논문</a>)
    <ul>
      <li>특정 네트워크의 weight들을 예측하는 네트워크로, 해당 네트워크가 특정 입력에 대해 더 expressive한 출력을 낼 수 있도록 해줍니다. 본 논문의 핵심적 역활을 합니다.</li>
    </ul>
  </li>
  <li>[58] : <a href="https://github.com/danielroich/PTI">PTI</a> (<a href="https://arxiv.org/abs/2106.05744">논문</a>)
    <ul>
      <li>latent vector optimization 으로 입력 이미지에 대한 latent vector를 구한 후, 이미지의 수정이 용이하도록 해당 latent vector에 대해 StyleGAN generator를 재훈련시키는 모델입니다. 이미지의 복원과 수정 양쪽에서 좋은 결과를 보여주지만, inference 시간이 느리다는 단점이 있으며, HyperStyle은 바로 이 점을 고치고자 합니다.</li>
    </ul>
  </li>
</ul>

<hr />
<h2 id="method">Method</h2>

<h3 id="선행연구">선행연구</h3>

<p>GAN-Inversion 문제를 Latent vector optimization 으로 풀 경우,</p>
<ul>
  <li>목표는 주어진 입력 이미지 / 훈련된 모델 (StyleGAN2를 전제로 함) / 손실 함수 (L2 또는 LPIPS) 가 있을 때, 손실 함수를 최소화 하는 latent vector를 찾는게 됩니다.</li>
  <li>이는 보통 단일 이미지에 대해 수분의 시간을 소요하게끔 하며, 따라서 비효율적입니다.</li>
</ul>

<p>위의 문제를 해결하기 위해 Encoder based optimization 을 사용할 경우,</p>
<ul>
  <li>여러 장의 훈련 이미지 데이터셋에 대해 손실함수를 최소화 시키는 방향으로 훈련해, inference시 입력 이미지에 대한 latent vector를 바로 출력하게끔 합니다.</li>
  <li>이 경우 몇초 정도만 걸리며, 이후 이미지를 수정시 별개의 latent manipulation 을 통해 수정된 latent vector를 StyleGAN2에 입력해서 원하는 수정된 이미지를 얻을 수 있습니다.</li>
  <li>그러나 latent vector optimization에 비해 이미지 복구는 상대적으로 질이 떨어집니다.</li>
</ul>

<p><a href="https://github.com/danielroich/PTI">PTI</a>는 encoder와는 다르게, StyleGAN2 generator를 수정해서 목적을 달성합니다.</p>
<ul>
  <li>입력 이미지에 대한 latent vector optimization을 통해 estimated latent vector를 구하고, 이 latent vector에 대해 손실함수를 최소화 시킬 수 있는 StyleGAN2 weight를 구해, 이미지 복원 / 수정간 균형을 맞추게 합니다 (이때 latent vector는 고정됩니다).</li>
  <li>그러나 latent vector optimization이 기본 골자이며, StyleGAN2 weight optimzation 에 대해 걸리는 시간도 길기 때문에 해당 방법은 역시 비효율적 입니다.</li>
</ul>

<hr />
<h3 id="overview">Overview</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/model-overview.png" alt="모델 구조" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">HyperStyle 모델 구조 : 입력 이미지를 ReStyle-e4e에 넣어 latent vector를 얻은뒤, StyleGAN에 넣어서 복원된 이미지를 얻습니다. 이후 입력 이미지와 복원된 이미지를 HyperNetwork에 넣어, StyleGAN conv layers에 대한 weight offset을 얻습니다. 그리고 latent vector와 weight offset을 적용시키면 (더 잘) 복원된 이미지를 얻게 됩니다.</td>
    </tr>
  </tbody>
</table>

<p>저자들이 제시한 HyperStyle은 PTI 에 Encoder-based 방법론을 적용한 후, HyperNetwork를 StyleGAN2 weight optimzation 대신 사용해서 목표를 달성합니다.</p>

<p>우선, estimated latent vector 를 구할 땐 (비교적) 이미지 수정에 용이한 방향으로 latent vector를 출력하는 <a href="https://github.com/omertov/encoder4editing">e4e</a>을 사용합니다.</p>
<ul>
  <li>초기 e4e 모델의 선택이 중요합니다.</li>
  <li>(당연하지만) latent vector에 대한 별개의 수정은 없기 때문에 기존 e4e에 사용하는 이미지 수정 방법론들을 그대로 사용할 수 있습니다.</li>
</ul>

<p>latent vector를 구한 후, PTI 처럼 StyleGAN에 대한 <strong>직접적인 optimization 을 하는게 아닌</strong>, 주어진 이미지(들)에 대해 최적의 StyleGAN weight 를 구하는 HyperNetwork H를 사용한 후, 이 weight들과 latent vector를 통해 구한 이미지가 원래 이미지와 같도록 훈련합니다.</p>
<ul>
  <li>이때 HyperNetwork의 입력은 원 입력 이미지와, latent vector를 넣어 StyleGAN에서 나온 이미지 두개를 넣습니다.</li>
</ul>

<p>엄밀히 말하자면, HyperNetwork는 StyleGAN weight 그 자체를 구하는게 아닌, 입력값에 대한 최적의 weight offset를 구하고, inference 시에는 <a href="https://github.com/yuval-alaluf/restyle-encoder">ReStyle</a> 의 원리를 본따, <strong>순차적으로 weight offset을 통해 StyleGAN을 수정</strong>합니다.(default : 2~5번)</p>

<p>풀어서 해석하면, HyperStyle은 <strong>단일 이미지 (그리고 단일 latent vector)에 대해 최적의 StyleGAN generator를 배우는거</strong>지만, 일반적인 이미지와는 다르게 ReStyle 와 크게 차이나지 않는 수준으로 inference 속도가 빠르며, 이미지의 복원/수정간 균형도 잘 잡는 모습을 보입니다.</p>

<hr />

<h3 id="hypernetwork-에-대하여">HyperNetwork 에 대하여</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/hyperstyle-refinement.png" alt="모델 구조" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">HyperStyle 안 Shared Refinement Block (위) 와 Refinement Block (아래)의 구조들입니다. 둘 다 channel_in * channel_out * 1 * 1 크기의 weight offset을 출력하며, Shared Refinement Block의 경우 공유되는 소 네트워크를 가져 information sharing을 가능케 합니다.</td>
    </tr>
  </tbody>
</table>

<p>StyleGAN의 모델 변수 갯수는 약 3천만개가 넘으며, 모든 변수들에 대해서 weight offset을 구하는 hypernetwork는 그 변수가 약 30억개를 넘을 예정입니다. 따라서 효율과 실효성 사이의 균형을 맞추기 위한 네트워크 디자인이 중요해집니다.</p>

<p>우선, HyperNetwork는 ResNet34를 골자로 삼고, 그 목표는 StyleGAN의 convolutional layer들의 weight parameter offset를 배우는걸로 합니다.</p>

<ul>
  <li>
    <p>여기서 affine transformation layer들의 경우, 어차피 배워야 할 latent vector는 하나이기 때문에 convolutional layer weight를 바꾸는 걸로 affine transformation을 대신할 수 있기 때문에, 굳이 배우지 않습니다.</p>
  </li>
  <li>
    <p>To-RGB 레이어들의 경우, <a href="https://github.com/betterze/StyleSpace">StyleSpace</a> 논문에 따르면 해당 레이어들을 바꾸는건 pixel-wise texture / color 구조를 바꾸기 때문에, 역시 수정하는 것을 권장하지 않습니다.</p>
  </li>
  <li>
    <p>conv.layer 들 또한, 선택에 따라 중/후반부의 conv. layer들 만을 선택할 수 있고, 모두 선택할 수도 있습니다.</p>
  </li>
  <li>
    <p>따라서 <strong>바꾸는건 (해상도 1024 기준) StyleGAN의 convolutional layer 26개의 conv.weight로 한정되며</strong>, HyperNetwork에는 각 stylegan conv. layer마다 ResNet34의 출력값을 바탕으로 conv.layer 에 대응하는 weight offset을 출력하는 Refinement Block를 둡니다.</p>
  </li>
</ul>

<p>HyperNetwork에 있어 가장 중요한 네트워크 디자인 결정은 바로 <strong>channel-wise offset</strong>를 구한다는 겁니다.</p>

<ul>
  <li>
    <p>예를 들어서, conv.weight 크기가 [512, 512, 3, 3] (in_channel=out_channel=512, kernel_size=3)인 경우, 해당 레이어의 Refinement Block 는 [512, 512, 1, 1] 사이즈의 weight offset을 출력하고, 이를 [3, 3]으로 반복해서 커널에 대해 똑같은 값을 더합니다.</p>
  </li>
  <li>
    <p>해당 결정으로 88% 가량의 HyperNetwork parameter를 줄일수 있었다고 저자들은 밝히며, 이로 인한 이미지 복원 손상은 발생하지 않았다고 합니다 (다만 channel-wise offset을 해야만 하는 결정적인 이슈는 따로 없는거 같습니다)</p>
  </li>
</ul>

<p>여기서 변수들의 숫자를 더 줄이기 위해 <strong>Shared Refinement Block</strong>가 따로 사용됩니다.</p>
<ul>
  <li>
    <p>다른 Refinement Block들과는 달리 Shared Refinement Block들은 블록의 마지막에 두개의 FC layer들이 있고, 이 레이어들은 모든 Shared Refinement Block에 걸쳐 그 weight가 공유됩니다.</p>
  </li>
  <li>
    <p>이 특성상 레이어들간의 겹치는 정보를 공유할 수 있고, 필요한 변수들의 숫자를 줄이면서 이미지 복원의 질을 향상시킬 수 있습니다.</p>
  </li>
  <li>
    <p>거시적인 구조들이 초반에 생성되고 미시적인 디테일들이 후반에 잡히는 StyleGAN의 구조상, 해당 Shared Refinement Block들은 초반 conv layer (weight 사이즈가 [512,512,3,3] 인 첫 4개 가량의 레이어들)에 대해서만 사용됩니다.</p>
  </li>
</ul>

<p>이 디자인 결정들을 통해, HyperNetwork는 변수들의 숫자를 크게 아껴 inference를 보다 용이하게 합니다. 마지막으로, ReStyle에서 제시한 순차적 refinement를 따와, HyperStyle을 여러번 돌려 최적의 stylegan weight offset을 계산할 수 있도록 합니다.</p>

<hr />

<h3 id="훈련-절차">훈련 절차</h3>

<ul>
  <li>데이터셋 : [인간] : FFHQ, CelebA-HQ, [동물, 사물] : Stanford Cars, AFHQ Wild</li>
  <li>손실함수 : L2 + LPIPS percept.loss + ID Loss
    <ul>
      <li>대상이 인간인 경우, ArcFace 기반 ID-loss (pSp에서 쓴것과 동일합니다) 를 사용</li>
      <li>대상이 인간이 아닐 경우, MoCo 기반 similarity loss 사용</li>
    </ul>
  </li>
</ul>

<h3 id="결과--이미지-복원">결과 : 이미지 복원</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/hyperstyle-recon.png" alt="이미지 복원" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">HyperStyle와 타 모델들의 이미지 복원 능력. direct optimization, PTI와 HyperStyle이 가장 근접한 모습을 보이며, HyperStyle이 가장 선명한 모습을 보입니다.</td>
    </tr>
  </tbody>
</table>

<p>HyperStyle은 latent vector optimization 과 동등한 수준의 이미지 복원을 선보이면서, 몇배 빠른 속도를 보여줍니다.</p>
<ul>
  <li>특히 PTI의 경우, 저해상도 이미지의 복원시 이미지에 overfitting하려는 경향 때문에 상대적으로 뿌연 결과를 보여주는 반면, HyperStyle은 Encoder-based 의 latent vector를 사용하면서 그런 해상도에 대한 방해요소가 적습니다.</li>
</ul>

<p>pSp이나 e4e같은 인코더 기반 모델과 비교해도 HyperStyle의 성능이 더 좋으며 ReStyle과도 비슷한 수준의 결과를 보여줍니다.</p>

<p>정량적으로 L2-distance, LPIPS-distance, MS-SSIM 같은 수치들을 inference time 대비 비교해봤을떄, HyperStyle의 성능이 타 모델을 크게 뛰어넘는걸 볼 수 있고, 이를 통해 HyperStyle은 latent vector optimzation의 이미지 복원 성능과 encoder-based optimization의 효율성을 동시에 갖췄다고 할 수 있습니다.</p>

<table>
  <thead>
    <tr>
      <th>모델</th>
      <th>ID</th>
      <th>MS-SSIM</th>
      <th>LPIPS</th>
      <th>L2</th>
      <th>Inf.Time(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>StyleGAN2</td>
      <td>0.78</td>
      <td>0.90</td>
      <td>0.09</td>
      <td>0.020</td>
      <td>227.55</td>
    </tr>
    <tr>
      <td>PTI</td>
      <td>0.85</td>
      <td>0.92</td>
      <td>0.09</td>
      <td>0.015</td>
      <td>55.715</td>
    </tr>
    <tr>
      <td>IDInvert</td>
      <td>0.18</td>
      <td>0.68</td>
      <td>0.22</td>
      <td>0.061</td>
      <td>0.04</td>
    </tr>
    <tr>
      <td>pSp</td>
      <td>0.56</td>
      <td>0.76</td>
      <td>0.17</td>
      <td>0.034</td>
      <td>0.106</td>
    </tr>
    <tr>
      <td>e4e</td>
      <td>0.50</td>
      <td>0.72</td>
      <td>0.20</td>
      <td>0.052</td>
      <td>0.106</td>
    </tr>
    <tr>
      <td>ReStyle-pSp</td>
      <td>0.66</td>
      <td>0.79</td>
      <td>0.13</td>
      <td>0.030</td>
      <td>0.366</td>
    </tr>
    <tr>
      <td>ReStyle-e4e</td>
      <td>0.52</td>
      <td>0.74</td>
      <td>0.19</td>
      <td>0.041</td>
      <td>0.366</td>
    </tr>
    <tr>
      <td>HyperStyle</td>
      <td>0.76</td>
      <td>0.84</td>
      <td>0.09</td>
      <td>0.019</td>
      <td>1.234</td>
    </tr>
  </tbody>
</table>

<p>이미지 복원에 대한 정량적 지표들입니다. HyperStyle은 PTI와 비슷하거나 조금 부족한 수치들을 보이지만 속도에 있어서 수십배 빠르고, 기존 인코더 모델 대비 시간이 조금 느린 대신 더 좋은 수치를 보입니다.</p>
<h3 id="결과--이미지-수정">결과 : 이미지 수정</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/hyperstyle-edit.png" alt="이미지 복원" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">HyperStyle와 타 모델들의 이미지 수정 능력. HyperStyle이 인물, 비인물 가리지 않고 가장 원 identity를 유지하면서 자연스러운 수정이 가해지는 모습을 보입니다.</td>
    </tr>
  </tbody>
</table>

<p>이미지 수정 범위의 경우, pSp나 ReStyle-pSp는 latent space에서 수정이 어려운 부분들에 위치한 latent code를 내놓기 때문에 의미있는 수정이 어렵습니다. e4e나 ReStyle-e4e는 수정이 가능한 범위가 좀 넓지만, 원본 대상의 identity preservance가 어렵습니다.</p>

<p>그러나 HyperStyle은 PTI처럼 latent space에서 수정이 용이한 부분에 위치한 latent code를 제공하기 때문에 원본의 identity를 유지하면서 넓은 폭의 수정이 가능합니다.</p>

<p>HopeNet, Anycost, CurricularFace 등의 네트워크 등을 통해 얻은 수치들을 다양한 step size를 놓고 성능을 비교해 봤을때, HyperStyle이 시간 대비 효율과 이미지 복원, 이미지 수정 모두에 대해 SOTA급 성능을 보여줌을 확인할 수 있습니다.</p>
<ul>
  <li>다양한 방법들로 구한 latent vector에 동일한 step size를 통한 editing을 해봐야 서로 다른 세기로 변할 것이므로, 여러가지 step size에 대해 부드러운 editing이 가능한지를 놓고 비교합니다</li>
</ul>

<h3 id="결과--abalation-study">결과 : Abalation Study</h3>
<ul>
  <li>StyleGAN 네트워크 상의 coarse layer를 제외한 medium-fine layer들만 HyperNetwork의 대상으로 놓고 훈련시켰을시 속도도 빨라지면서 이미지 복원 성능도 유지하는 모습을 보입니다.
    <ul>
      <li>그러나 코드 상에선 일단 모든 레이어들에 대해 학습합니다.</li>
    </ul>
  </li>
  <li>To-RGB 레이어들을 HyperNetwork로 수정하려고 하면 이미지 수정 능력이 현저히 떨어짐을 확인할 수 있습니다.</li>
  <li>ReStyle같은 순차적인 HyperNetwork 개선으로, 이미지 방해요소들을 상당히 제거할 수 있습니다.</li>
  <li>Shared Refinement Block을 써서 레이어간 information sharing을 원할히 할 수 있습니다.</li>
  <li>Separable Convolution (커널 전체에 대해 같은 offset을 내놓는 channel-wise offset이 아닌, 커널의 행 따로 열 따로 offset을 계산후 곱해서 최종 offset을 제공하는 형태로, 이를 통해 속도가 느려지지만 network expressiveness를 키운다)를 사용하는게 크게 성능을 늘려주지 못합니다.</li>
</ul>

<h3 id="결과--다른-task에-대해서">결과 : 다른 task에 대해서</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/hyperstyle-domain.png" alt="도메인 혼합" /></th>
      <th style="text-align: center"><img src="/assets/images/hyperstyle/hyperstyle-out.png" alt="도메인 혼합" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Domain Adaption 비교</td>
      <td style="text-align: center">Out-of-domain 적용 비교</td>
    </tr>
  </tbody>
</table>

<p>Domain Adaption (예시: 실제 인물사진을 웹툰 도메인의 캐릭터로 바꾸는 태스크) 의 경우, StyleAlign 같은 모델들은 변환 과정에서 머리카락이나 기타 색상의 보존이 되지 않는 모습을 보이지만 HyperStyle의 경우, 해당 디테일들을 보다 잘 유지할 수 있습니다.</p>

<p>변환 과정 :</p>
<ul>
  <li>원 입력 도메인 (예: 인물사진) 에 대해 훈련시킨 source StyleGAN 모델과 그 모델에서부터 원하는 출력 도메인 (예: 웹툰 캐릭터)를 fine-tuning/layer sharing 시킨 target StyleGAN, 그리고 source StyleGAN 에 대해 훈련된 e4e, HyperStyle 모델이 있다고 가정합니다.</li>
  <li>입력 이미지를 우선 source-e4e로 넣어서 latent vector를 구하고, source-HyperStyle에 넣어서 weight offset을 구합니다.</li>
  <li>그리고 해당 offset을 target StyleGAN에 적용시킨 후에, latent vector를 넣어서 이미지를 구합니다.</li>
</ul>

<p>또한, 기존에 훈련했던 도메인에서 벗어난 도메인 입력 (예 : 실제 인물사진 모델에 웹툰 캐릭터 인코딩)의 경우에도, 타 모델들과는 달리 HyperStyle은 StyleGAN모델 그 자체를 더 맞게 바꿔주기 때문에, 보다 잘 적용되는 모습을 보입니다.</p>

<hr />

<h3 id="결론">결론</h3>

<p>HyperStyle은 이미지 복원/수정 간의 균형을 잘 잡아줄 뿐만 아니라, 빠른 시간안에 결과물을 출력해주면서 다양한 입력에 잘 적용되는 모습을 보여줍니다. 해당 논문의 방향성을 잘 연구해서 StyleGAN3 같은 논문에 적용시키거나, 여러 방식으로 훈련을 하면 highly reliable한 GAN-Inversion method이 될 거라고 생각됩니다.</p>]]></content><author><name></name></author><category term="blog" /><category term="stylegan" /><category term="gan-inversion" /><summary type="html"><![CDATA[Hyperstyle (StyleGAN Inversion with HyperNetworks for Real Image Editing)]]></summary></entry><entry><title type="html">Denoising Diffusion Probabilistic Models</title><link href="https://yj7082126.github.io/blog/2021/DDPM/" rel="alternate" type="text/html" title="Denoising Diffusion Probabilistic Models" /><published>2021-10-19T10:10:00+00:00</published><updated>2021-10-19T10:10:00+00:00</updated><id>https://yj7082126.github.io/blog/2021/DDPM</id><content type="html" xml:base="https://yj7082126.github.io/blog/2021/DDPM/"><![CDATA[<h2 id="diffusion-model-이란">Diffusion Model 이란</h2>

<blockquote>
  <p>데이터셋의 이미지들에 작은 노이즈를 주입하는 과정들로 구성된 “정방향 프로세스”가 있을 때,
해당 프로세스의 반대인 “역방향 프로세스”를 배워, 노이즈로부터 데이터셋 분포에 포함된 샘플을 생성하는 모델. </p>
</blockquote>

<p>예를 들어, CelebA 데이터셋에서 뽑은 얼굴 $x_0$가 있다고 가정하겠습니다.
CelebA 의 데이터셋 분포를 $q(x)$ 라고 하면, $x_0 \sim q(x)$ 라고 할 수 있겠죠.</p>

<p>이 $x_0$ 에 극소량의 노이즈 (가우시안으로 고정)을 추가하는 방법을 무한히 반복하면,
T 시점의 데이터 $x_T$ 는 원래 얼굴을 알아볼 수 없는, 가우시안 분포 $\mathcal{N}(0, \mathbf{I})$ 에 속해있을 수 있습니다.</p>

<p>이론상은 이 노이즈 추가를 무한히 반복해야겠지만,
실제로는 노이즈 삽입을 250~1,000회 이상 실행하는 것으로 기존 데이터를 노이즈로 바꿀 수 있습니다.</p>

<p>Diffusion 모델은 바로 이 노이즈 삽입 과정의 역을,
즉 이미지와 노이즈가 섞인 데이터에서 극소량의 노이즈를 제거하는 과정을 배움으로써,
가우시안 분포에서 데이터셋 분포로 바꾸는 작업을 수행할 수 있게 됩니다.</p>

<h2 id="forward-diffusion-process-정과정">Forward Diffusion Process (정과정)</h2>

<p>정과정 에서는 최초 데이터 $x_0$ 에서부터 노이즈를 더하는 방식으로  $x_1, x_2, …, x_T$ 의 샘플들을 생성하며, 
이 과정에 각 스텝은 변수 ${\beta_t \in (0,1)}^T_{t=1}$ 로 통제됩니다.
후반부의 스텝으로 갈 수록 안전하게 큰 노이즈를 추가할 수 있기 때문에, 
$\beta$를 점진적으로 증가하는 구조로 만들 수 있습니다. ($\beta_1 &lt; \beta_2 &lt; … &lt; \beta_T$)</p>

<p>또한, 정과정은 마르코브 체인의 형태로 구성됩니다; $x_t$ 시점의 분포는, $x_{t-1}$ 에 의해서만 전제됩니다.</p>

<blockquote>
  <p>\(q(x_{t} | x_{t-1}) \approx \mathcal{N}(x_{t}; \sqrt{1 - \beta_t}x_{t-1}, \beta_t\mathbf{I}) \\
q(x_{1:T}|x_0) \approx \prod^{T}_{t=1} q(x_t|x_{t-1})\)
(평균값을 줄이고, 표준편차를 늘리면서 가우시안 정규분포에 가까워진다.)</p>
</blockquote>

<p>이 조건부 분포에서, VAE에서 쓴 reparameterization trick을 쓰면 $x_{t-1}$를 통해 $x_t$를 구할 수 있습니다.
($x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}z$)</p>

<p>그리고 마르코브 체인에 따라 분포들을 합쳐가다 보면, $x_0$과 $t$가 있으면 를 $x_t$구할 수 있습니다.
편의를 위해 $\alpha_t = 1 - \beta_t$, $\bar{\alpha_t} = \prod^{t}_{s=1}\alpha_s$ 로 쓰면</p>

<blockquote>
  <p>\(\begin{align*}
x_t =&amp; \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z \\ 
=&amp; \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t}\alpha_{t-1}}z \\
=&amp; \text{ } ... \\
=&amp; \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}z \\
\end{align*}\)
\(q(x_t|x_0) \approx \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1 - \bar{\alpha_t})\mathbf{I})\)
으로 식을 전개할 수 있음을 확인할 수 있습니다.</p>
</blockquote>

<h2 id="reverse-diffusion-process-역과정">Reverse Diffusion Process (역과정)</h2>

<p>만약 저희가 $q(x_{t-1}|x_t)$를 알 수 있으면, 가우시안 노이즈 $q(x_T)$에서 출발해, 데이터셋에 맞는 이미지를 생산할 수 있습니다.
그걸 알 수 없기 때문에 딥러닝으로 해당 분포를 근사해야만 합니다.
이 $q(x_{t-1}|x_t)$ 의 “근사 함수” 를 $p_{\theta}(x_{t-1}|x_t)$로 정의합니다. ($\theta$ : 파라미터 값)</p>

<h2 id="training-loss">Training Loss</h2>

<h2 id="ddpm-parameterization">DDPM Parameterization</h2>

<h2 id="training-and-sampling">Training and Sampling</h2>

<h2 id="case-of-l_0">Case of $L_0$</h2>

<h2 id="참고문헌">참고문헌</h2>
<p><img src="https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html" alt="Lilian Weng : What are Diffusion Models?" /></p>]]></content><author><name></name></author><category term="blog" /><category term="diffusion" /><summary type="html"><![CDATA[Diffusion Model 이란]]></summary></entry></feed>