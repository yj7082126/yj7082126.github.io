---
title: NeRF - Representing Scenes as Neural Radiance Fields for View Synthesis
date: 2021-07-21T15:10:00-04:00  
categories:
-   blog  
tags:
-   computer vision
-   NeRF
---

### Introduction
* By optimizing volumetric scene function using sparse set of input views, we can synthesize novel views of complex scenes.
* Input : Scenes consisted of continuous *spatial locations* $(x, y, z)$ and *viewing directions* $(\theta, \phi)$
* Output : *Volume density* and *view-dependent color* at spatial location.
	* Density : differential opacity controlling the amount of radiance accumulated by a ray passing thru position.
* Model : MLP, without convolutional layers.
	* Because basic implementation does not converge to sufficient representation, we use the following;
		* We transform the input coordinates with positional encoding, to represent higher frequency functions.
		* We propose hierarchical sampling procedure to reduce number of queries.
* Using traditional volume rendering techniques, we can project the output into synthesized images.

### Related Works : Neural 3D Shape representations.
* Implicit representation of 3D shapes as level sets
	* Method #1 : (x, y, z) coords to signed distance functions.
		* Curless, B : [A volumetric method for building complex models from range images](https://www.semanticscholar.org/paper/A-volumetric-method-for-building-complex-models-Curless-Levoy/07f254f33e4c99d4c443d9a5d9221f1e9af6c106)
		* Jiang, C : [Local implicit grid representations for 3d scenes](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/projects/local_implicit_grid/README.md)
		* Park, J : [DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://github.com/facebookresearch/DeepSDF)
	* Method #2 : (x, y, z) coords to occupancy fields.
		* Genova, K : [Local Deep Implicit Functions for 3D Shape](https://github.com/google/ldif)
		* Mescheder, L : [Occupancy networks: Learning 3D reconstruction in function space](https://github.com/autonomousvision/occupancy_networks)
	* Limited by requirement of ground truth 3D geometry (ex : [ShapeNet](https://shapenet.org/))
	* Recent : relax this requirement by functions that allow neural implicit shape representations to be optimized through 2D images.
		* Niemeyer, M : [Differentiable volumetric rendering](https://github.com/autonomousvision/differentiable_volumetric_rendering)
		* Sitzmann, V : [Scene representation networks](https://github.com/vsitzmann/scene-representation-networks)
		* Still limited to simple shapes.

### Related Works : View synthesis and image-based rendering.
* Light field sample interpolation
	* Levoy, M : [Light field rendering](https://www.cs.princeton.edu/courses/archive/fall12/cos526/papers/levoy06.pdf)
	
### NeRF Representation
* Input : 
	* 3D Location $\mathbf{x} = (x, y, z)$
	* 2D Viewing direction $\mathbf{d} = (\theta, \phi)$
* Output : 
	* Emitted color $\mathbf{c} = (r, g, b)$
	* Volume density $\sigma$
* Network
	* $F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$
	* In order to make the representation **multiview consistent**, restrict the network such that **volume density $\sigma$ is only predicted by location $x$**, regardless of direction.
		* $\mathbf{x} \rightarrow MLP(*8, 256) \rightarrow (\sigma, f.v)$
		* $Concat(f.v, \mathbf{d}) \rightarrow MLP(*1, 128) \rightarrow (\mathbf{c})$

### Volume Rendering with Radiance Field


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1MzYzMjQwNTcsLTE5OTEyODY3MTIsMj
kzNjQ3MDA1LC0xODgwMjI4NDMyLC02MDQ0NDAyNjgsLTIwODIx
Njg1MzUsMzk1NDA2MDcsMTAzMTQ0NzkwNV19
-->