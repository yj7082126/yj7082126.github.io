---
title: Denoising Diffusion Probabilistic Models
date: 2021-10-19T06:10:00-04:00  
categories:
-   blog  
tags:
-   diffusion
---

## Diffusion Model 이란

> 데이터셋의 이미지들에 작은 노이즈를 주입하는 과정들로 구성된 "정방향 프로세스"가 있을 때,
> 해당 프로세스의 반대인 "역방향 프로세스"를 배워, 노이즈로부터 데이터셋 분포에 포함된 샘플을 생성하는 모델. 

예를 들어, CelebA 데이터셋에서 뽑은 얼굴 $x_0$가 있다고 가정하겠습니다.
CelebA 의 데이터셋 분포를 $q(x)$ 라고 하면, $x_0 \sim q(x)$ 라고 할 수 있겠죠.

이 $x_0$ 에 극소량의 노이즈 (가우시안으로 고정)을 추가하는 방법을 무한히 반복하면,
T 시점의 데이터 $x_T$ 는 원래 얼굴을 알아볼 수 없는, 가우시안 분포 $\mathcal{N}(0, \mathbf{I})$ 에 속해있을 수 있습니다.

이론상은 이 노이즈 추가를 무한히 반복해야겠지만,
실제로는 노이즈 삽입을 250~1,000회 이상 실행하는 것으로 기존 데이터를 노이즈로 바꿀 수 있습니다.

Diffusion 모델은 바로 이 노이즈 삽입 과정의 역을,
즉 이미지와 노이즈가 섞인 데이터에서 극소량의 노이즈를 제거하는 과정을 배움으로써,
가우시안 분포에서 데이터셋 분포로 바꾸는 작업을 수행할 수 있게 됩니다.

## Forward Diffusion Process (정과정)

정과정 에서는 최초 데이터 $x_0$ 에서부터 노이즈를 더하는 방식으로  $x_1, x_2, ..., x_T$ 의 샘플들을 생성하며, 
이 과정에 각 스텝은 변수 ${\beta_t \in (0,1)}^T_{t=1}$ 로 통제됩니다.
후반부의 스텝으로 갈 수록 안전하게 큰 노이즈를 추가할 수 있기 때문에, 
$\beta$를 점진적으로 증가하는 구조로 만들 수 있습니다. ($\beta_1 < \beta_2 < ... < \beta_T$)

또한, 정과정은 마르코브 체인의 형태로 구성됩니다; $x_t$ 시점의 분포는, $x_{t-1}$ 에 의해서만 전제됩니다.

> $$
q(x_{t} | x_{t-1}) \approx \mathcal{N}(x_{t}; \sqrt{1 - \beta_t}x_{t-1}, \beta_t\mathbf{I}) \\
q(x_{1:T}|x_0) \approx \prod^{T}_{t=1} q(x_t|x_{t-1})
$$
(평균값을 줄이고, 표준편차를 늘리면서 가우시안 정규분포에 가까워진다.)

이 조건부 분포에서, VAE에서 쓴 reparameterization trick을 쓰면 $x_{t-1}$를 통해 $x_t$를 구할 수 있습니다.
($x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}z$)

그리고 마르코브 체인에 따라 분포들을 합쳐가다 보면, $x_0$과 $t$가 있으면 를 $x_t$구할 수 있습니다.
편의를 위해 $\alpha_t = 1 - \beta_t$, $\bar{\alpha_t} = \prod^{t}_{s=1}\alpha_s$ 로 쓰면

> $$ \begin{align*}
x_t =& \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z \\ 
=& \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t}\alpha_{t-1}}z \\
=& \text{ } ... \\
=& \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}z \\
\end{align*} $$
> $$q(x_t|x_0) \approx \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1 - \bar{\alpha_t})\mathbf{I})$$
으로 식을 전개할 수 있음을 확인할 수 있습니다.

## Reverse Diffusion Process (역과정)

만약 저희가 $q(x_{t-1}|x_t)$를 알 수 있으면, 가우시안 노이즈 $q(x_T)$에서 출발해, 데이터셋에 맞는 이미지를 생산할 수 있습니다.
그걸 알 수 없기 때문에 딥러닝으로 해당 분포를 근사해야만 합니다.
이 $q(x_{t-1}|x_t)$ 의 "근사 함수" 를 $p_{\theta}(x_{t-1}|x_t)$로 정의합니다. ($\theta$ : 파라미터 값)


## Training Loss

## DDPM Parameterization

## Training and Sampling

## Case of $L_0$

## 참고문헌
![Lilian Weng : What are Diffusion Models?](https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html)