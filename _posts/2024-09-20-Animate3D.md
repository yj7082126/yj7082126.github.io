---
layout: post
title: '[Paper Review] Animate3D: Animating Any 3D Model with Multi-view Video Diffusion'
date: 2024-09-20T09:10:00-04:00  
categories: blog  
tags: 3D VDM
---

## Animate3D: Animating Any 3D Model with Multi-view Video Diffusion

Authors : Quan Meng, Lei Li, Matthias Nießner, Angela Dai 
(CASIA, DAMO Academy, Alibaba Group, Hupan Lab)

[[Project Page]](https://animate3d.github.io/) 
[[Paper]](https://arxiv.org/pdf/2407.11398) 

Despite various research in **dynamic 3D content generation** (4D Generation) there hasn't been a singular foundation model. Separately learning spatial factors from 3D models and temporal motions from video models result in quality degradation (e.g. SVD + Zero-123), and animating 3D objects usually fails to preserve multi-view attributes.

Animate3D suggests to animate any 3D models with unified spatiotemporal consistent supervision. The process first starts with MV-VDM, a foundational 4D model based from MVDream and a spatiotemporal motion module, focused on learning natural dynamic motions. A MV2V-Adapter, adapted from I2V-Adapter, is also used to handle multi-view image conditions. For 3D context, 4DGS is jointly optimized through both reconstruction and 4D Score Distillation Sampling. For training, the authors also create MV-Video, a large-scale multi-view video dataset that consists about 1.8M multi-view videos.



### Citation

```
@misc{meng2024lt3sdlatenttrees3d,
	title={LT3SD: Latent Trees for 3D Scene Diffusion}, 
	author={Quan Meng and Lei Li and Matthias Nießner and Angela Dai},
	journal={arXiv preprint arXiv:2409.08215},
	year={2024}
}
```